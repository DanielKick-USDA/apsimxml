[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "apsimxml",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "apsimxml"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "apsimxml",
    "section": "Install",
    "text": "Install\npip install apsimxml",
    "crumbs": [
      "apsimxml"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "apsimxml",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "apsimxml"
    ]
  },
  {
    "objectID": "01_sql_to_parquet.html",
    "href": "01_sql_to_parquet.html",
    "title": "apsimxml",
    "section": "",
    "text": "import os, sqlite3\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n\n\ndbs_to_index = os.listdir('/home/Shared/cultivar_sim_exps')\ndbs_to_index = [e for e in dbs_to_index if e.split('.')[-1] == 'sqlite']\n\n\ndef get_sql_tables(\n        sql_path = f\"/home/Shared/cultivar_sim_exps/{'sim_1698728407_78912.sqlite'}\",\n        table_list = ['Ids'] #'DefaultCultivarsAll', 'Genotypes', 'Results'\n        ):\n    db = sqlite3.connect(sql_path)\n    return [pd.read_sql_query(f\"SELECT * from {table_name}\", db) for table_name in table_list]\n\n\nprint('Updating shared tables')\n# expect this to be the same table over and over\nDefaultCultivarsAll = [get_sql_tables(\n        sql_path = f\"/home/Shared/cultivar_sim_exps/{db_name}\",\n        table_list = ['DefaultCultivarsAll']\n        )[0].assign(File = db_name) for db_name in dbs_to_index]\n\nDefaultCultivarsAll = pd.concat(DefaultCultivarsAll)\n\nDefaultCultivarsAll = DefaultCultivarsAll.drop(columns = 'File').drop_duplicates()\nDefaultCultivarsAll.shape\n\n\nexpect_dupe = []\nexpect_uniq = []\n\nfor db_name in dbs_to_index:\n        db = sqlite3.connect(f\"/home/Shared/cultivar_sim_exps/{db_name}\")\n        if expect_dupe == []:\n            dupe = pd.read_sql_query(f\"SELECT * FROM Genotypes WHERE Genotype NOT LIKE 'Cultivar%'\", db)\n            expect_dupe.append(dupe.assign(File = db_name) )\n        expect_uniq.append(pd.read_sql_query(f\"SELECT * FROM Genotypes WHERE Genotype     LIKE 'Cultivar%'\", db).assign(File = db_name))\n\nGenotypes = pd.concat(expect_dupe+expect_uniq)\nGenotypes.File = Genotypes.File.str.replace('.sqlite', '')\n# Reorder cols\nGenotypes = Genotypes.loc[:, ['File', 'Genotype']+[e for e in list(Genotypes) if e not in ['File', 'Genotype']]]\ndel expect_dupe\ndel expect_uniq\nGenotypes.shape\n\n\nIds = [get_sql_tables(\n        sql_path = f\"/home/Shared/cultivar_sim_exps/{db_name}\",\n        table_list = ['Ids']\n        )[0].assign(File = db_name) for db_name in dbs_to_index]\n\nIds = pd.concat(Ids)\nIds.SoilIdx = Ids.SoilIdx.astype(int)\nIds.File = Ids.File.str.replace('.sqlite', '')\n# Reorder cols\nIds = Ids.loc[:, ['File']+[e for e in list(Ids) if e not in ['File']]]\nIds.shape\n\n\n# DefaultCultivarsAll.head()\n\n\n# Genotypes.head()\n\n\n# Ids.head()\n\n\n# save out global referenc tables as parquet files. \n# check if files exist if so load and extend. \ndef update_parquet_table(\n        base_path = \"/home/Shared/cultivar_sim_exps/\",\n        name = 'DefaultCultivarsAll',\n        table = DefaultCultivarsAll\n        ):\n    file_path = f\"{base_path}{name}.parquet\"\n    if os.path.exists(file_path):\n        old_table = pq.read_table(file_path).to_pandas()\n        table = old_table.merge(table)\n    table = pa.Table.from_pandas(table)\n    pq.write_table(table, file_path)\n\nfor k, v in zip(['DefaultCultivarsAll', 'Genotypes', 'Ids'], \n                [ DefaultCultivarsAll,   Genotypes,   Ids]):\n    update_parquet_table(\n        base_path = \"/home/Shared/cultivar_sim_exps/\",\n        name =  k,\n        table = v\n        )\n\n\n# now for each of the files with results, convert and delete original sqlite\n\ndef results_sqlite_to_parquet(file_path = f\"/home/Shared/cultivar_sim_exps/{'sim_1698728407_78912.sqlite'}\",\n                              delete_after = False,\n                              double_dare = False # ignore file size requirement\n                              ):\n    # cut off sqlite, add parquet\n    new_filepath = file_path[0:-6]+'parquet'\n\n    table = get_sql_tables(sql_path = file_path, table_list = ['Results'] )[0]\n    table.Date = table.Date.astype(int)\n    table = pa.Table.from_pandas(table)\n    pq.write_table(table, new_filepath)\n\n    if delete_after:\n        if os.path.exists(new_filepath):\n            if os.path.getsize(new_filepath) &gt; 1000000000:\n                del_bool = True\n            elif double_dare == True:\n                del_bool = True\n        if del_bool:\n            os.remove(file_path)\n\n\n# sort from smallest to largest\ntmp = {os.path.getsize(f\"/home/Shared/cultivar_sim_exps/{db}\"): db for db in dbs_to_index}\ntmp = [tmp[e] for e in sorted( list(tmp.keys()) )]\n\n\nprint('staring conversion process')\nfor db in tmp:\n    gb = round(os.path.getsize(f\"/home/Shared/cultivar_sim_exps/{db}\")/1000000000, 2)\n    print(f\"{gb}G\\t| {db}\")\n    results_sqlite_to_parquet(file_path = f\"/home/Shared/cultivar_sim_exps/{db}\",\n                            delete_after = False,\n                            double_dare = False # ignore file size requirement\n                            )\n\n\n# pq.read_table(f\"/home/Shared/cultivar_sim_exps/{'sim_cultivar.parquet'}\").to_pandas()\n\n\n# pq.read_table(f\"/home/Shared/cultivar_sim_exps/{'sim_1697187607_79518.parquet'}\").to_pandas()\n\n\n# pq.read_table(f\"/home/Shared/cultivar_sim_exps/{'sim_1698310807_76163.parquet'}\").to_pandas()\n\n\n# len(dbs_to_index)\n# for db in dbs_to_index[0:1]:",
    "crumbs": [
      "01_sql_to_parquet.html"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "core",
    "section": "",
    "text": "source\n\nfoo\n\n foo ()",
    "crumbs": [
      "core"
    ]
  },
  {
    "objectID": "02_parquet_dataloader.html",
    "href": "02_parquet_dataloader.html",
    "title": "apsimxml",
    "section": "",
    "text": "import os\nimport numpy as np\nimport pandas as pd\nimport einops\nimport torch \n\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\n\nfrom tqdm import tqdm\n\nimport pyarrow as pa\nimport pyarrow.parquet as pq\nimport datetime\n\n# Compute unix epoch to date table\n\ndef _prep_unix_epoch_to_date(max_year = 2025):\n    month_abbr = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n    max_day = (datetime.datetime(max_year, 1, 1, 0, 0) - datetime.datetime(1970, 1, 1, 0, 0)).days \n    unix_epoch = [i for i in range(max_day)]\n    date_times = [datetime.datetime(1970, 1, 1, 0, 0) + datetime.timedelta(i) for i in range(max_day)]\n\n    def as_SowDate(date_time):\n        day = f'{date_time.day}'\n        if len(day) == 1:\n            return(f'0{day}-{month_abbr[date_time.month - 1]}')\n        else:\n            return(f'{day}-{month_abbr[date_time.month - 1]}')\n\n    tmp = pd.DataFrame({\n        'Unix': unix_epoch,\n        'Year':[e.year for e in date_times],\n        'Month':[e.month for e in date_times],\n        'Day':[e.day for e in date_times],\n        'SowDate':[as_SowDate(date_time = e) for e in date_times]\n    })\n    _ = tmp.loc[:, ['Unix', 'Year']].groupby(['Year']).min().reset_index().rename(columns={'Unix':'MinUnix'})\n    tmp = tmp.merge(_)\n\n    tmp['DOY'] = tmp['Unix'] - tmp['MinUnix']\n    tmp = tmp.drop(columns = ['MinUnix'])\n    return tmp\n# how bad would it be if we stored a bunch of tiny parquet files? \n# saving many tiny parquet files will increase storage cost by 1.6x\n# That's not great but not horrible either. We're talking about approximately 500 gb.\n# (305213583096*1.6)/1000000000\n# 488.34\n\n# apsimx_sim_parquet_dir = '/home/Shared/cultivar_sim_exps'\n# Result = pq.read_table(apsimx_sim_parquet_dir+'/'+'sim_1698440407_4739.parquet').to_pandas()\n\n# -rw-rw-r-- 1 kickd newgroup 2939581425 Jun  1 01:08 cultivar_sim_exps/sim_1698440407_4739.parquet\n\n\n# for i in tqdm(Result.FactorialUID.unique()):\n#     table = Result.loc[(Result.FactorialUID == i), ].drop(columns = 'FactorialUID')\n#     table = pa.Table.from_pandas(table)\n#     pq.write_table(table, f'/home/Shared/testing_rm_after0620/{i}.parquet')\n\n# 100%|██████████| 24025/24025 [33:58&lt;00:00, 11.79it/s]\n# The goal is to have a (postgres?) SQL db that we can query. To not have a delay we're going to instead load a batch into memory BUT allow for redefining this batch by swapping out the dataloader.\n\n# Workflow:\n# Define the desired data \n# Use the main tables to figure out what parquet files we need to pull from.\n# Pull all the data in and represent as tensors\napsimx_sim_parquet_dir = '/home/Shared/cultivar_sim_exps'\nos.listdir(apsimx_sim_parquet_dir)[0:3]\n\n['sim_1698440407_4739.parquet',\n 'sim_1697418008_10643.parquet',\n 'sim_1697187607_79518.parquet']\n[e for e in os.listdir(apsimx_sim_parquet_dir) if e[0:4] != 'sim_']\n\n['DefaultCultivarsAll.parquet', 'Genotypes.parquet', 'Ids.parquet']\n# metadata \nDefaultCultivarsAll = pq.read_table(apsimx_sim_parquet_dir+'/'+'DefaultCultivarsAll.parquet').to_pandas()\nGenotypes           = pq.read_table(apsimx_sim_parquet_dir+'/'+'Genotypes.parquet').to_pandas()\nIds                 = pq.read_table(apsimx_sim_parquet_dir+'/'+'Ids.parquet').to_pandas()\n# I'm setting up a class to help find the files we need to read through to build the datset \n# This works by holding a copy of the Ids and Genotypes tables. \n# We'll operate on those, filtering them down until the tables only contain the enries we want to use.\n# Next we'll use a that produces tuples of the (parquet file, filtering criteria)\n\nclass data_helper():\n    def __init__(self, genotypes_path, ids_path, results_path, ssurgo_path, met_path):\n        # used later to get the results. Append / if there isn't one.\n        if results_path[-1] != '/': \n            results_path = results_path+'/'\n        self.results_path = results_path\n\n        Genotypes = pq.read_table(genotypes_path).to_pandas()\n        # coerce None to NaN (default cultivars don't have all values specified)\n        for e in [ee for ee in list(Genotypes) if ee not in ['File', 'Genotype']]:\n            Genotypes[e] = Genotypes[e].astype(float)\n        mask = (Genotypes.isna().sum(axis = 1) == 0)\n\n        self.Genotypes = Genotypes.loc[mask, ].reset_index(drop = True)\n        self.Ids = pq.read_table(ids_path).to_pandas()\n\n        # Environmental data\n        self.ssurgo_data = pq.read_table(ssurgo_path\n                            ).to_pandas(\n                            ).rename(columns = {'soil_i':'SoilIdx'})\n        \n        self.met_data    = pq.read_table(met_path\n                            ).to_pandas(\n                            ).rename(columns = {'latitude':  'Latitude',\n                                                'longitude': 'Longitude'})\n\n    \n    def apply_mask(self, table, mask):\n        \"This method takes care of automatically filtering the non-masked table.\"\n        if table not in ['Genotypes', 'Ids']:\n            print('table should be Genotypes or Ids')   \n        else:\n            if table == 'Genotypes':\n                # apply mask to filter the table\n                self.Genotypes = self.Genotypes.loc[mask, ].reset_index(drop = True)\n                # left join to update the other table\n                self.Ids = self.Genotypes.loc[:, ['File', 'Genotype']\n                                        ].drop_duplicates(\n                                        ).merge(self.Ids, how = 'left'\n                                        ).reset_index(drop = True)\n            elif table == 'Ids':\n                self.Ids = self.Ids.loc[mask, ].reset_index(drop = True)\n                self.Genotypes = self.Ids.loc[:, ['File', 'Genotype']\n                                    ].drop_duplicates(\n                                    ).merge(self.Genotypes, how = 'left'\n                                    ).reset_index(drop = True)\n    def load_results(self, years = [], dry_run = True):\n        self.years = years\n        # Now we can ask for the files that we should get\n        tmp = self.Ids.loc[:, ['File', 'Genotype', 'FactorialUID']]\n\n        get_files = tmp.File.drop_duplicates().to_list()\n        print(f'{len(get_files)} files to be read.')\n\n        if dry_run == True:\n            print('In dry_run, reading no files')\n\n        if dry_run == False:\n            res_list = []\n            col_order = ''\n            for file in tqdm(get_files):\n                # print(f'{file}')\n                res = pq.read_table(f'{x.results_path+file}.parquet').to_pandas()\n\n                # columns should be in the same order, but we will force them to be here. \n                if col_order == '':\n                    col_order = list(res)\n\n                # filter order established with some informal testing.\n                # runtime filter years, factorials: [10, 11.3, 10.5]\n                # runtime filter factorials, years: [9.0, 8.4, 9.6]\n\n                # filter factorials\n                res = tmp.loc[(tmp.File == file), ['FactorialUID']].merge(res)\n\n                # filter years\n                if years != []:\n                    yr = _prep_unix_epoch_to_date(max_year = 2030)\n                    yr = yr.loc[(yr.Year.isin(years)), ['Unix']].rename(columns={'Unix':'Date'})\n                    res = yr.merge(res)\n\n                res_list.append(res)\n            res_list = pd.concat(res_list).reset_index(drop=True)\n            self.results = res_list\n\n    def setup_encoders(self):\n        # create string to num encoders so we can use tensors for all lookups\n        Ids = self.Ids\n        self.encoder_Genotype = {k:v for k,v in zip(Ids.Genotype.unique(), range(len(Ids.Genotype.unique())))}\n        self.encoder_File     = {k:v for k,v in zip(Ids.File.unique(),     range(len(Ids.File.unique())))}\n        self.encoder_SowDate  = {k:v for k,v in zip(Ids.SowDate.unique(),  range(len(Ids.SowDate.unique())))}\n\n    def results_as_arrays(self, output_type = 'point'): #output_type = 'sequence'\n        metadata = ['FactorialUID', 'Year']\n        metrics  = ['Maize.AboveGround.Wt', 'Maize.LAI', 'yield_Kgha']\n        ref = {\n            'lookup_names': metadata,\n            'data_names': metrics,\n        }\n\n        # Turn results into tensor\n        tmp = self.results.merge(_prep_unix_epoch_to_date(max_year = 2025).loc[:, ['Unix', 'Year', 'DOY']].rename(columns={'Unix':'Date'}))\n\n        # find the smallest above ground weight within each year, usethat to get the first date within eacy year and then filter for the values that are &gt;= the within year start.\n        _ = tmp.loc[:, metadata+['Maize.AboveGround.Wt']].groupby(metadata).agg('min').reset_index()\n        _ = _.merge(tmp).loc[:, metadata+['Date']].rename(columns={'Date': 'YearStart'})\n        tmp = tmp.merge(_, how='left')\n\n        tmp = tmp.loc[(tmp.Date &gt;= tmp.YearStart), ]\n\n        tmp=tmp.drop(columns=['YearStart', 'Date']\n            ).drop_duplicates(\n            ).sort_values(metadata+['DOY']\n            ).reset_index(drop = True)\n\n        if output_type not in ['point', 'sequence']:\n            print('output_type expected to be point or sequence')\n        if output_type == 'point':\n            ref['data_dims'] = ['obs', 'metrics']\n            # if making point estimates:\n            tmp = tmp.drop(columns='DOY').groupby(metadata).agg('max').reset_index()\n\n            lookup = torch.tensor(tmp.loc[:, metadata].to_numpy())\n            out = torch.tensor(tmp.drop(columns=metadata).to_numpy())\n        \n        if output_type == 'sequence':\n            ref['data_dims'] = ['obs', 'days', 'metrics']\n            day_lookup = _prep_unix_epoch_to_date(max_year = 2025).rename(columns={'Unix':'Date'})\n            # day_lookup\n            lookup = tmp.loc[:, metadata].drop_duplicates().reset_index(drop = True)\n            \n            # obs, channels, length\n            out = torch.zeros((lookup.shape[0], \n                            365,\n                            3\n                            ))\n            \n            for i in tqdm(range(lookup.shape[0])):\n                # break\n                uid, yr = lookup.loc[i, ]\n\n                # use Ids table to get planting date. \n                sow_date = self.Ids.loc[(self.Ids.FactorialUID == uid), 'SowDate']\n\n                plant_DOY = day_lookup.loc[(\n                    (day_lookup.Year == yr) & \n                    (day_lookup.SowDate == sow_date.values[0])), 'DOY'].values[0]\n\n\n                mask = (\n                    (tmp.FactorialUID == uid) & \n                    (tmp.Year == yr) &\n                    (tmp.DOY &gt;= plant_DOY) & \n                    (tmp.DOY &lt;= 364) # because there are leap years and indexing starts at 0, clip to the minimum of these\n                    )\n                \n                start_DOY= tmp.loc[mask, 'DOY'].min()\n                stop_DOY = tmp.loc[mask, 'DOY'].max()\n\n                out[i, start_DOY:(stop_DOY+1), :] = torch.tensor(tmp.loc[mask, metrics].to_numpy())\n                # # propagate forward observations to all dates following the max\n                out[i, stop_DOY:365, :] = out[i, stop_DOY, :]\n            \n        return(lookup, out, ref)  \n\n\n    # Working with Environmental Data\n    def filter_env(self):\n        # filter to selected years\n        if self.years != []:\n            self.met_data = self.met_data.loc[(self.met_data.year.isin(self.years)), ].reset_index(drop = True)\n\n        # shared soilIdx\n        Ids_idxs = self.Ids.loc[:, ['Longitude', 'Latitude', 'SoilIdx']].drop_duplicates()\n        ssurgo_s = self.ssurgo_data.loc[:, ['SoilIdx']].drop_duplicates()\n        met_lonlat = self.met_data.loc[:, ['Longitude', 'Latitude']].drop_duplicates()\n\n        print(f'Uniq. Lon/Lat/Soil: {Ids_idxs.shape[0]}')\n        print('Condition on env availability:')\n        Ids_idxs = Ids_idxs.merge(ssurgo_s, how = 'inner').merge(met_lonlat, how = 'inner')\n        print(f'Uniq. Lon/Lat/Soil: {Ids_idxs.shape[0]}')\n\n        # reduce the environmental datasets:\n        self.met_data    = Ids_idxs.drop(columns=['SoilIdx']).drop_duplicates().merge(self.met_data, how = 'inner')\n        self.ssurgo_data = Ids_idxs.drop(columns=['Longitude', 'Latitude']).drop_duplicates().merge(self.ssurgo_data, how = 'inner')\n\n        # update Ids & results:\n        self.Ids = Ids_idxs.merge(self.Ids)\n        _ = self.Ids.loc[:, ['FactorialUID']].drop_duplicates()\n        self.results = _.merge(self.results, how='inner')\n\n        # refresh index on everything\n        self.Ids         = self.Ids.reset_index(        drop = True)\n        self.results     = self.results.reset_index(    drop = True)\n        self.Genotypes   = self.Genotypes.reset_index(  drop = True)\n        self.met_data    = self.met_data.reset_index(   drop = True)\n        self.ssurgo_data = self.ssurgo_data.reset_index(drop = True)\n\n    def met_as_arrays(self, ops_string = 's d m -&gt; s d m'): # site day metric\n        m = self.met_data.copy()\n        m = m.rename(columns = {'year': 'Year'})\n        metadata = [\n            'Longitude', \n            'Latitude', \n            'Year'\n            ]\n        metrics = [\n            'radn_MJ_div_m2_div_day',\n            'maxt_oC',\n            'mint_oC',\n            'rain_mm',\n            'rh_pr',\n            'windspeed_m_div_s',\n            'tav',\n            'amp'\n            ]\n\n\n        # make sure that there are the expected number of values per group\n        m = m.loc[(m.day &lt; 366), ].sort_values(metadata).reset_index(drop = True)\n        # there's at least one site that appears to have at least two records. I'll deal with this by (tav and amp appear to be slightly different)\n        # see m.loc[((m.Longitude == -86.529600) & (m.Latitude == 34.729520) & (m.year == 2014) ), ].drop_duplicates().sort_values('day')\n\n        m = m.groupby(metadata+['day']).mean().reset_index()\n\n        # all groups have 365 obs?\n        assert False not in (m.groupby(metadata).count().reset_index().loc[:, ['day']] == 365).values\n\n        lookup = m.loc[:, metadata].drop_duplicates().to_numpy()\n\n        m = m.loc[:, metrics\n            ].to_numpy(\n            ).reshape(\n                -1,           # as many sites as are available\n                365,          # days\n                len(metrics)) # metrics\n\n\n        m = einops.rearrange(m, ops_string)\n\n        # let's also allow for an operations string so that we can _request_ data in a specific format instead of changing it after the fact.\n        ops = ops_string.replace(' ', '').split('-&gt;')\n        dim_order = [i\n        for e in ops[1]\n        for i in range(len(ops[0]))\n        if e == ops[0][i]\n        ]\n\n        ref = {\n            'lookup_names': metadata,\n            'data_names': metrics,\n            'data_dims': [['site', 'days', 'metrics'][i] for i in dim_order] # use the ops string to figure out the order that will be returned\n        }\n\n        return (lookup, m, ref)\n\n    def ssurgo_as_arrays(self, ops_string = 's l m -&gt; s l m'): # site lenght (depth) metric\n        s = self.ssurgo_data.copy()\n\n        metadata = [\n            'SoilIdx',\n            ]\n        metrics = [\n            'BD',\n            'AirDry',\n            'LL15',\n            'DUL',\n            'SAT',\n            'KS',\n            'Carbon',\n            'SoilCNRatio',\n            'FOM',\n            'FOM.CN',\n            'FBiom',\n            'FInert',\n            'NO3N',\n            'NH4N',\n            'PH',\n            'ParticleSizeClay',\n            'ParticleSizeSilt',\n            'ParticleSizeSand',\n            'Maize.KL',\n            'Maize.LL',\n            'Maize.XF',\n            # 'Soybean.KL',\n            # 'Soybean.LL',\n            # 'Soybean.XF',\n            # 'Wheat.KL',\n            # 'Wheat.LL',\n            # 'Wheat.XF'\n            ]\n\n        # check that the soil slices are equally sized\n        assert len(s.Thickness.unique()) == 1\n        print(f'Soil compartment thickness: {s.Thickness[0]}')\n\n        # check that there are an equal number of slices\n        _  = s.loc[:, ['SoilIdx', 'Thickness']].groupby(['SoilIdx']).count().reset_index()\n        assert _.Thickness.min() == _.Thickness.max()\n\n        # turn range into start of slice\n        s['Depth'] = s['Depth'].str.split(pat = '-', expand = True)[0].astype(int)\n        s = s.sort_values(['SoilIdx', 'Depth']).reset_index(drop = True)\n\n        lookup = s.loc[:, metadata].drop_duplicates().to_numpy()\n\n        s = s.loc[:, metrics].to_numpy()\n        s = s.reshape(-1, lookup.shape[0], len(metrics))\n\n\n        s = einops.rearrange(s, ops_string)\n\n        # let's also allow for an operations string so that we can _request_ data in a specific format instead of changing it after the fact.\n        ops = ops_string.replace(' ', '').split('-&gt;')\n        dim_order = [i\n        for e in ops[1]\n        for i in range(len(ops[0]))\n        if e == ops[0][i]\n        ]\n\n\n        ref = {\n            'lookup_names': metadata,\n            'data_names': metrics,\n            'data_dims': [['site', 'depth', 'metrics'][i] for i in dim_order] # use the ops string to figure out the order that will be returned\n        }\n        return (lookup, s, ref)\n    \n    def genotype_as_arrays(self):\n        g = self.Genotypes.copy()\n\n        # Genotypes are easy to deal with because the table is cleaned up in filtering results. \n        metadata = [\n            'File',\n            'Genotype',\n            ]\n        metrics = [\n            'Grain.MaximumGrainsPerCob.FixedValue',\n            'Grain.MaximumPotentialGrainSize.FixedValue',\n            'Phenology.FlagLeafToFlowering.Target.FixedValue',\n            'Phenology.FloweringToGrainFilling.Target.FixedValue',\n            'Phenology.GrainFilling.Target.FixedValue',\n            'Phenology.Juvenile.Target.FixedValue',\n            'Phenology.Maturing.Target.FixedValue',\n            'Phenology.MaturityToHarvestRipe.Target.FixedValue',\n            'Phenology.Photosensitive.Target.XYPairs.X__1',\n            'Phenology.Photosensitive.Target.XYPairs.X__2',\n            'Phenology.Photosensitive.Target.XYPairs.X__3',\n            'Phenology.Photosensitive.Target.XYPairs.Y__1',\n            'Phenology.Photosensitive.Target.XYPairs.Y__2',\n            'Phenology.Photosensitive.Target.XYPairs.Y__3',\n            'Rachis.DMDemands.Structural.DMDemandFunction.MaximumOrganWt.FixedValue',\n        ]\n\n        lookup = g.loc[:, metadata]\n        lookup.File     = [self.encoder_File[e]     for e in lookup.File]\n        lookup.Genotype = [self.encoder_Genotype[e] for e in lookup.Genotype]\n        lookup = lookup.to_numpy()\n\n        g = g.loc[:, metrics].to_numpy()\n\n        ref = {\n            'lookup_names': metadata,\n            'data_names': metrics,\n            'data_dims': ('genotype', 'metrics')\n        }\n        return(lookup, g, ref)\n    \n    def ids_as_arrays(self):\n        Ids =  self.Ids.copy()\n        # apply encoder\n        Ids.Genotype = [self.encoder_Genotype[e] for e in Ids.Genotype]\n        Ids.File     = [self.encoder_File[e]     for e in Ids.File]\n        Ids.SowDate  = [self.encoder_SowDate[e]  for e in Ids.SowDate]\n\n        ref = {'lookup_names': list(Ids),}\n        Ids = Ids.to_numpy()\n        return(Ids, ref)\n    \n    def sowing_as_array(self):\n        # precompute Year/SowDate -&gt; Planting DOY\n        _ = pd.DataFrame([(y, s) for y in self.years for s in self.encoder_SowDate], columns=['Year', 'SowDate'])\n        # Add in DOY\n        _=_.merge(_prep_unix_epoch_to_date(max_year = 2025).loc[:, ['Year', 'SowDate', 'DOY']])\n        # overwrite SowDate with encoded version\n\n        _.SowDate = [x.encoder_SowDate[e] for e in _.SowDate]\n\n        ref = {\n            'lookup_names': list(_)\n        }\n        planting_lookup = _.to_numpy()\n        \n        return(planting_lookup, ref)\n\nx = data_helper(\n    genotypes_path = apsimx_sim_parquet_dir+'/'+'Genotypes.parquet',\n    ids_path = apsimx_sim_parquet_dir+'/'+'Ids.parquet',\n    results_path = apsimx_sim_parquet_dir,\n    ssurgo_path = '/home/Shared/apsimx_env_data/apsimx_i_soil_ssurgo_profile.parquet',\n    met_path    = '/home/Shared/apsimx_env_data/apsimx_i_soil_POWER_met.parquet'\n    )\n\n# restrict to simulated cultivars with the maximum MaximumGrainsPerCob\nmask = (x.Genotypes['Grain.MaximumGrainsPerCob.FixedValue'] == x.Genotypes['Grain.MaximumGrainsPerCob.FixedValue'].max())\n\nprint(f'Before Mask: Genotypes: {x.Genotypes.shape} Ids: {x.Ids.shape}')\nx.apply_mask(table='Genotypes', mask=mask)\nprint(f'After Mask: Genotypes: {x.Genotypes.shape} Ids: {x.Ids.shape}')\n\nx.setup_encoders()\nx.load_results(years = [1990, 2000, 2010, 2020], dry_run = False)\nx.filter_env()\n\nBefore Mask: Genotypes: (3150, 17) Ids: (2837275, 7)\nAfter Mask: Genotypes: (5, 17) Ids: (4247, 7)\n5 files to be read.\nUniq. Lon/Lat/Soil: 126\nCondition on env availability:\nUniq. Lon/Lat/Soil: 109\n\n\n100%|██████████| 5/5 [00:25&lt;00:00,  5.12s/it]\nresult_lookup,   result,   result_ref   = x.results_as_arrays(output_type = 'point')\n\ngenotype_lookup, genotype, genotype_ref = x.genotype_as_arrays()\nmet_lookup,      met,      met_ref      = x.met_as_arrays(   ops_string = 's d m -&gt; s d m')\nssurgo_lookup,   ssurgo,   ssurgo_ref   = x.ssurgo_as_arrays(ops_string = 's l m -&gt; s l m')\nids_lookup,                ids_ref      = x.ids_as_arrays()\nsow_lookup,                sow_ref      = x.sowing_as_array()\n\nSoil compartment thickness: 200.0\n# # precompute Year/SowDate -&gt; Planting DOY\n# _ = pd.DataFrame([(y, s) for y in x.years for s in x.encoder_SowDate], columns=['Year', 'SowDate'])\n# # Add in DOY\n# _=_.merge(_prep_unix_epoch_to_date(max_year = 2025).loc[:, ['Year', 'SowDate', 'DOY']])\n# # overwrite SowDate with encoded version\n\n# _.SowDate = [x.encoder_SowDate[e] for e in _.SowDate]\n# _\nclass apsimxDataset(Dataset):\n    def __init__(\n            self, \n            result_lookup,   result,\n            ids_lookup,\n            sow_lookup,\n            sow_met_concat, # TRUE\n            genotype_lookup, genotype,\n            met_lookup,      met,\n            ssurgo_lookup,   ssurgo,\n            ):\n        # super().__init__()\n        self.result_lookup = result_lookup\n        self.result = result\n        self.ids_lookup = ids_lookup\n        self.sow_lookup = sow_lookup\n        self.sow_met_concat = sow_met_concat\n        self.genotype_lookup = genotype_lookup\n        self.genotype = genotype\n        self.met_lookup = met_lookup\n        self.met = met\n        self.ssurgo_lookup = ssurgo_lookup\n        self.ssurgo = ssurgo\n\n    def __len__(self):\n        return len(self.result_lookup.shape[0])\n    \n    def __getitem__(self, idx):\n        # setup keys\n        # get keys from y\n        FactorialUID, Year = self.result_lookup[idx, ]\n        # get use ids to get env keys from Ids table\n        Longitude, Latitude, SoilIdx, File, Genotype, SowDate, FactorialUID = self.ids_lookup[(self.ids_lookup[:, 6] == int(FactorialUID)), ].squeeze()\n        # now we have the full set of keys.\n\n        # get output values\n        y = self.result[idx]\n\n        # cultivar variables\n        mask = (\n            (self.genotype_lookup[:, 0] == File) & \n            (self.genotype_lookup[:, 1] == Genotype)\n            )\n        cultivar = self.genotype[mask, ]\n\n        # ssurgo data\n        mask = (self.ssurgo_lookup[:, 0] == int(SoilIdx))\n        ssurgo = self.ssurgo[:, mask, :]\n\n        # met data\n        mask = (\n            (self.met_lookup[:, 0] == Longitude) & \n            (self.met_lookup[:, 1] == Latitude) & \n            (self.met_lookup[:, 2] == float(Year)) \n            )\n        met = self.met[mask, :, :]\n        if self.sow_met_concat:\n            # Concatenate a column with 0/1 for whether sowing has occured.\n            mask = (\n                (self.sow_lookup[:, 0] == Year) &\n                (self.sow_lookup[:, 1] == SowDate)\n                )\n            sow_doy = self.sow_lookup[mask, 2]\n            _ = torch.zeros([365])\n            _[sow_doy:] = 1\n            met = torch.concat([met, _[None, :, None]], axis = 2)\n\n        return (y, cultivar, ssurgo, met)\n\n\ndat = apsimxDataset(\n    result_lookup   = result_lookup,   \n    result          = result,\n    ids_lookup      = torch.tensor(ids_lookup),\n    sow_lookup      = torch.tensor(sow_lookup),\n    sow_met_concat  = True,\n    genotype_lookup = torch.tensor(genotype_lookup), \n    genotype        = torch.tensor(genotype),\n    met_lookup      = torch.tensor(met_lookup),      \n    met             = torch.tensor(met),\n    ssurgo_lookup   = torch.tensor(ssurgo_lookup),   \n    ssurgo          = torch.tensor(ssurgo),\n    )\n\n\ni_y, i_cultivar, i_ssurgo, i_met = next(iter(dat))\n# core of the dataloader\nresult_lookup.shape[0] # number of ys\n\ni = 0\n\n# y variable\nresult[i]\n\n# x variables\n## \n# get keys from y\nFactorialUID, Year = result_lookup[i, ]\n\n# get use ids to get env keys from Ids table\nLongitude, Latitude, SoilIdx, File, Genotype, SowDate, FactorialUID = Ids[(Ids[:, 6] == int(FactorialUID)), ].squeeze()\n\n# now we have the full set of keys.\n\n# cultivar variables\nmask = (\n    (genotype_lookup[:, 0] == File) & \n    (genotype_lookup[:, 1] == Genotype)\n    )\n\ngenotype[mask, ]\n\n# ssurgo data\nmask = (ssurgo_lookup[:, 0] == int(SoilIdx))\nssurgo[:, mask, :]\n\n# met data\n\nmask = (\n    (met_lookup[:, 0] == Longitude) & \n    (met_lookup[:, 1] == Latitude) & \n    (met_lookup[:, 2] == float(Year)) \n    )\n\nmet[mask, :, :]\n# recreate data_helper so that we can look at how the methods in data_helper work\nx = data_helper(\n    genotypes_path = apsimx_sim_parquet_dir+'/'+'Genotypes.parquet',\n    ids_path = apsimx_sim_parquet_dir+'/'+'Ids.parquet',\n    results_path = apsimx_sim_parquet_dir,\n    ssurgo_path = '/home/Shared/apsimx_env_data/apsimx_i_soil_ssurgo_profile.parquet',\n    met_path    = '/home/Shared/apsimx_env_data/apsimx_i_soil_POWER_met.parquet'\n    )\n\n# restrict to simulated cultivars with the maximum MaximumGrainsPerCob\nmask = (x.Genotypes['Grain.MaximumGrainsPerCob.FixedValue'] == x.Genotypes['Grain.MaximumGrainsPerCob.FixedValue'].max())\nx.apply_mask(table='Genotypes', mask=mask)\nm = x.met_data\nmetadata = [\n    'Longitude', \n    'Latitude', \n    'year'\n    ]\nmetrics = [\n    'radn_MJ_div_m2_div_day',\n    'maxt_oC',\n    'mint_oC',\n    'rain_mm',\n    'rh_pr',\n    'windspeed_m_div_s',\n    'tav',\n    'amp'\n    ]\n\n# make sure that there are the expected number of values per group\nm = m.loc[(m.day &lt; 366), ].sort_values(metadata).reset_index(drop = True)\nm = m.groupby(metadata+['day']).mean().reset_index()\n\n# all groups have 365 obs?\nassert False not in (m.groupby(metadata).count().reset_index().loc[:, ['day']] == 365).values\n# Demonstrate reshaping. We want to \"fold\" this table so that there is a site (lon/lat/year) axis, a day axis, and a metrics axis.\n\n# To demonstrate this we'll get the \nmn = m.loc[:, metadata+['day']].to_numpy()\n\nmn = mn.reshape(-1,  # as many sites as are available\n                365, # days\n                4)   # metrics (in this case lon, lat, year, day)\n\nimport plotly.express as px\nprint('Here we\\'ll show that this reshape is working as expected by plotting the \"day\" column across several sites')\npx.imshow(\n    np.concatenate([\n        mn[i, 0:10, 3:]\n        for i in range(10)\n    ], axis = 1)\n)\nm = x.met_data\nmetadata = [\n    'Longitude', \n    'Latitude', \n    'year'\n    ]\nmetrics = [\n    'radn_MJ_div_m2_div_day',\n    'maxt_oC',\n    'mint_oC',\n    'rain_mm',\n    'rh_pr',\n    'windspeed_m_div_s',\n    'tav',\n    'amp'\n    ]\n\n\n# make sure that there are the expected number of values per group\nm = m.loc[(m.day &lt; 366), ].sort_values(metadata).reset_index(drop = True)\nm = m.groupby(metadata+['day']).mean().reset_index()\n\n# all groups have 365 obs?\nassert False not in (m.groupby(metadata).count().reset_index().loc[:, ['day']] == 365).values\nm = m.loc[:, metrics\n    ].to_numpy(\n    ).reshape(\n        -1,           # as many sites as are available\n        365,          # days\n        len(metrics)) # metrics\n\nm[0, 0:5, :]\n# let's also allow for an operations string so that we can _request_ data in a specific format instead of changing it after the fact.\nops_string = 's d m -&gt; s m d'\n\nops = ops_string.replace(' ', '').split('-&gt;')\n\n\ndim_order = [i\n for e in ops[1]\n for i in range(len(ops[0]))\n if e == ops[0][i]\n ]\n\n\nm.shape, einops.rearrange(m, ops_string).shape\nref = {\n    'lookup_names': metadata,\n    'data_names': metrics,\n    'data_dims': [['site', 'days', 'metrics'][i] for i in dim_order] # use the ops string to figure out the order that will be returned\n}\n# to help avoid confustion later, we'll return some reference information too\n# (lookup, m, ref)\n# self.ssurgo_data = self.ssurgo_data.reset_index(drop = True)\ns = x.ssurgo_data\n\nmetadata = [\n    'SoilIdx',\n    ]\nmetrics = [\n    'BD',\n    'AirDry',\n    'LL15',\n    'DUL',\n    'SAT',\n    'KS',\n    'Carbon',\n    'SoilCNRatio',\n    'FOM',\n    'FOM.CN',\n    'FBiom',\n    'FInert',\n    'NO3N',\n    'NH4N',\n    'PH',\n    'ParticleSizeClay',\n    'ParticleSizeSilt',\n    'ParticleSizeSand',\n    'Maize.KL',\n    'Maize.LL',\n    'Maize.XF',\n    # 'Soybean.KL',\n    # 'Soybean.LL',\n    # 'Soybean.XF',\n    # 'Wheat.KL',\n    # 'Wheat.LL',\n    # 'Wheat.XF'\n    ]\n\ns.head()\n# check that the soil slices are equally sized\nassert len(s.Thickness.unique()) == 1\nprint(f'Soil compartment thickness: {s.Thickness[0]}')\n# check that there are an equal number of slices\n_  = s.loc[:, ['SoilIdx', 'Thickness']].groupby(['SoilIdx']).count().reset_index()\nassert _.Thickness.min() == _.Thickness.max()\n# turn range into start of slice\ns['Depth'] = s['Depth'].str.split(pat = '-', expand = True)[0].astype(int)\ns = s.sort_values(['SoilIdx', 'Depth']).reset_index(drop = True)\n\nlookup = s.loc[:, metadata].drop_duplicates().to_numpy()\n\ns = s.loc[:, metrics].to_numpy()\ns = s.reshape(-1, lookup.shape[0], len(metrics))\nref = {\n    'lookup_names': metadata,\n    'data_names': metrics,\n    'data_dims': ('site', 'depth', 'metrics')\n}\n# to help avoid confustion later, we'll return some reference information too\n# (lookup, s, ref)\n# self.Genotypes   = self.Genotypes.reset_index(  drop = True)\ng = x.Genotypes\n\n# Genotypes are easy to deal with because the table is cleaned up in filtering results. \n\ng.head()\nmetadata = [\n    'File',\n    'Genotype',\n    ]\nmetrics = [\n    'Grain.MaximumGrainsPerCob.FixedValue',\n    'Grain.MaximumPotentialGrainSize.FixedValue',\n    'Phenology.FlagLeafToFlowering.Target.FixedValue',\n    'Phenology.FloweringToGrainFilling.Target.FixedValue',\n    'Phenology.GrainFilling.Target.FixedValue',\n    'Phenology.Juvenile.Target.FixedValue',\n    'Phenology.Maturing.Target.FixedValue',\n    'Phenology.MaturityToHarvestRipe.Target.FixedValue',\n    'Phenology.Photosensitive.Target.XYPairs.X__1',\n    'Phenology.Photosensitive.Target.XYPairs.X__2',\n    'Phenology.Photosensitive.Target.XYPairs.X__3',\n    'Phenology.Photosensitive.Target.XYPairs.Y__1',\n    'Phenology.Photosensitive.Target.XYPairs.Y__2',\n    'Phenology.Photosensitive.Target.XYPairs.Y__3',\n    'Rachis.DMDemands.Structural.DMDemandFunction.MaximumOrganWt.FixedValue',\n]\nref = {\n    'lookup_names': metadata,\n    'data_names': metrics,\n    'data_dims': ('genotype', 'metrics')\n}\n# (g.loc[:, metadata].to_numpy(), g.loc[:, metrics].to_numpy(), ref)\n## Example of Internals:",
    "crumbs": [
      "Masking genotypes"
    ]
  },
  {
    "objectID": "02_parquet_dataloader.html#clean-up-environmental-data",
    "href": "02_parquet_dataloader.html#clean-up-environmental-data",
    "title": "apsimxml",
    "section": "Clean up Environmental Data",
    "text": "Clean up Environmental Data\n\nenv_path = '/home/Shared/apsimx_env_data'+'/'\n\n# ./apsimx_i_soil_ssurgo_profile.parquet\n\n# met files (easy to parse)\n# env_path+'apsimx_i_soil_POWER_met.parquet'\n\nlookup = pq.read_table(env_path+'apsimx_i_soil_gps.parquet').to_pandas()\n\n\n# load env data\nmet_data = pq.read_table(env_path+'apsimx_i_soil_POWER_met.parquet').to_pandas().rename(columns = {'latitude':'Latitude', \n                                                                                                   'longitude': 'Longitude'})\nmet_data\n\nssurgo_data = pq.read_table(env_path+'apsimx_i_soil_ssurgo_profile.parquet').to_pandas().rename(columns = {'soil_i':'SoilIdx'})\nssurgo_data\n\n\n# shared soilIdx\n\nIds_idxs = x.Ids.loc[:, ['Longitude', 'Latitude', 'SoilIdx']].drop_duplicates()\nssurgo_s = ssurgo_data.loc[:, ['SoilIdx']].drop_duplicates()\nmet_lonlat = met_data.loc[:, ['Longitude', 'Latitude']].drop_duplicates()\n\n\nprint(f'Uniq. Lon/Lat/Soil: {Ids_idxs.shape[0]}')\nprint('Condition on data availability:')\nIds_idxs = Ids_idxs.merge(ssurgo_s, how = 'inner').merge(met_lonlat, how = 'inner')\nprint(f'Uniq. Lon/Lat/Soil: {Ids_idxs.shape[0]}')\n\n\n# reduce the environmental datasets:\nmet_data    = Ids_idxs.drop(columns=['SoilIdx']).drop_duplicates().merge(met_lonlat, how = 'inner')\nssurgo_data = Ids_idxs.drop(columns=['Longitude', 'Latitude']).drop_duplicates().merge(ssurgo_data, how = 'inner')\n\n\n# update Ids & results:\nx.Ids = Ids_idxs.merge(x.Ids)\n\n_ = x.Ids.loc[:, ['FactorialUID']].drop_duplicates()\nx.results = _.merge(x.results, how='inner')\n\n\nIds_s    = x.Ids.loc[:, ['SoilIdx']].drop_duplicates()\nx.Ids\n\n\nIds\n\n\n# using inner join here to make sure there aren't any indexes in the results that we don't have environmental data for\nssurgo_data.merge(Ids_s, how='inner')\n\n\nIds_s.merge(ssurgo_data, how='inner')\n\n\nPhysical Properties\n\nenv_tables = [\n    'apsimx_i_soil_Soils_CERESSoilTemperature.parquet',\n    'apsimx_i_soil_Soils_Chemical.parquet',\n    'apsimx_i_soil_Soils_Nutrients.Nutrient.parquet',\n    'apsimx_i_soil_Soils_Organic.parquet',\n    'apsimx_i_soil_Soils_Physical.parquet',\n    'apsimx_i_soil_Soils_Soil.parquet',\n    'apsimx_i_soil_Soils_Solute.parquet',\n    'apsimx_i_soil_Soils_Water.parquet',\n    'apsimx_i_soil_WaterModel_WaterBalance.parquet']\n\nsoil_dict = {e:pq.read_table(env_path+e).to_pandas() for e in env_tables}#[env_tables[1]]\n\n# focus on only one entry\nsoil_dict = {k:soil_dict[k].loc[(soil_dict[k].list_idx == 1), ] for k in env_tables}\n\n\nsorted(list(set(sum([list(pq.read_table(env_path+e).to_pandas()) for e in env_tables], []))))\n\n# problem: There are some columns that just don't look like they are saved in the data. E.g. soil/Physical.`maize LL` \n\n['$type',\n 'AirDry',\n 'AirDryMetadata',\n 'ApsoilNumber',\n 'BD',\n 'BDMetadata',\n 'CN2Bare',\n 'CNCov',\n 'CNRed',\n 'Carbon',\n 'CarbonUnits',\n 'CatchmentArea',\n 'Comments',\n 'Country',\n 'D0',\n 'DUL',\n 'DULMetadata',\n 'DataSource',\n 'DepthConstant',\n 'DiffusConst',\n 'DiffusSlope',\n 'DischargeWidth',\n 'Enabled',\n 'Exco',\n 'FBiom',\n 'FIP',\n 'FInert',\n 'FOM',\n 'FOMCNRatio',\n 'FilledFromTop',\n 'InitialPAWmm',\n 'InitialValues',\n 'InitialValuesUnits',\n 'KS',\n 'LL15',\n 'LL15Metadata',\n 'Latitude',\n 'Longitude',\n 'MaxDepthSoluteAccessible',\n 'MaxEffectiveRunoff',\n 'Name',\n 'PH',\n 'PHMetadata',\n 'PHUnits',\n 'PSIDul',\n 'ParticleSizeClay',\n 'ParticleSizeSand',\n 'ParticleSizeSilt',\n 'ReadOnly',\n 'RecordNumber',\n 'Region',\n 'RelativeTo',\n 'ResourceName',\n 'RunoffEffectivenessAtMovingSolute',\n 'SAT',\n 'SATMetadata',\n 'SWCON',\n 'Salb',\n 'SoilCNRatio',\n 'SoilType',\n 'State',\n 'SummerCona',\n 'SummerDate',\n 'SummerU',\n 'Thickness',\n 'WaterTableConcentration',\n 'WinterCona',\n 'WinterDate',\n 'WinterU',\n 'YearOfSampling',\n 'list_idx']\n\n\nsoil_dict['apsimx_i_soil_Soils_Physical.parquet']\n\n\n# table to cols:\ndesired_fields = {\n# 'apsimx_i_soil_Soils_CERESSoilTemperature.parquet': [],\n'apsimx_i_soil_Soils_Chemical.parquet': ['Thickness', 'PH'],\n# 'apsimx_i_soil_Soils_Nutrients.Nutrient.parquet': [],\n'apsimx_i_soil_Soils_Organic.parquet': ['Thickness', 'Carbon', 'SoilCNRatio', 'FBiom', 'FInert', 'FOM', 'FOMCNRatio', 'CarbonUnits'],\n'apsimx_i_soil_Soils_Physical.parquet': ['Thickness', 'ParticleSizeClay', 'ParticleSizeSand', 'ParticleSizeSilt', 'BD', 'AirDry', 'LL15', 'DUL', 'SAT'],\n# 'apsimx_i_soil_Soils_Soil.parquet': [],\n# 'apsimx_i_soil_Soils_Solute.parquet': [],\n'apsimx_i_soil_Soils_Water.parquet': ['Thickness', 'InitialValues', 'InitialPAWmm', 'RelativeTo'],\n'apsimx_i_soil_WaterModel_WaterBalance.parquet': ['Thickness', 'SWCON']\n}\n\n\nres = [soil_dict[k].loc[:, desired_fields[k]] for i, k in enumerate(desired_fields)]\n\n\n# pd.merge( res[1])\nres[0]\nres[1]\n\n\nsoil_dict[env_tables[1]]    \nsoil_dict[env_tables[3]]    \n\n# soil_dict['apsimx_i_soil_Soils_CERESSoilTemperature.parquet']\nsoil_dict['apsimx_i_soil_Soils_Chemical.parquet'] # Thickness   PH \n# soil_dict['apsimx_i_soil_Soils_Nutrients.Nutrient.parquet']\nsoil_dict['apsimx_i_soil_Soils_Organic.parquet'] #Thickness Carbon  SoilCNRatio FBiom   FInert  FOM $type   FOMCNRatio  CarbonUnits\nsoil_dict['apsimx_i_soil_Soils_Physical.parquet'] #     Thickness   ParticleSizeClay    ParticleSizeSand    ParticleSizeSilt    BD  AirDry  LL15    DUL SAT \n# soil_dict['apsimx_i_soil_Soils_Soil.parquet'] # contains metadata\n# soil_dict['apsimx_i_soil_Soils_Solute.parquet'] # # Maybe useful, but it dosn't look like it? Thickness   InitialValues   Exco    FIP     InitialValuesUnits  WaterTableConcentration D0  DepthConstant   MaxDepthSoluteAccessible    RunoffEffectivenessAtMovingSolute   MaxEffectiveRunoff  Name\nsoil_dict['apsimx_i_soil_Soils_Water.parquet'] # Thickness  InitialValues   $type   InitialPAWmm    RelativeTo\nsoil_dict['apsimx_i_soil_WaterModel_WaterBalance.parquet'] #    Thickness   SWCON\n\n\n# Depth Thickness   BD  AirDry  LL15    DUL SAT KS  Carbon  SoilCNRatio FOM FOM.CN  FBiom   FInert  NO3N    NH4N    PH  ParticleSizeClay    ParticleSizeSilt    ParticleSizeSand    Maize.KL    Maize.LL    Maize.XF    Soybean.KL  Soybean.LL  Soybean.XF  Wheat.KL    Wheat.LL    Wheat.XF",
    "crumbs": [
      "Masking genotypes"
    ]
  },
  {
    "objectID": "02_parquet_dataloader.html#define-working-set",
    "href": "02_parquet_dataloader.html#define-working-set",
    "title": "apsimxml",
    "section": "Define working set",
    "text": "Define working set\n\n# Starting with a location and year\n\nimport plotly.express as px\n\nfig = px.scatter_mapbox(Ids.loc[:, ['Longitude', 'Latitude']].drop_duplicates(), lon = 'Longitude', lat = 'Latitude',\n                        color_discrete_sequence=[\"fuchsia\"], zoom=3, height=300)\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()\n\n\n# -&gt; File   Genotype  --Genotypes-&gt; \n#    FactorialUID     --Results-&gt;\n\nlon, lat, soil = [-76.65611874999999, 42.733264, 141] #lon lat soil\n\n# should allow ranges, slices, or all\nsow = '19-Jun'\n# allow all\ncultivar = 'Cultivar1'\n\n\n# -&gt; File FactorialUID\n\nmask = (\n    (Ids.Longitude == lon) &\n    (Ids.Latitude == lat) &\n    (Ids.SoilIdx == soil) &\n    (Ids.SowDate == sow) &\n    (Ids.Genotype == cultivar))\n\nIds.loc[mask, ]\n\n\n# Starting with a target set of genotypes\nmask = Genotypes['Grain.MaximumGrainsPerCob.FixedValue'] == Genotypes['Grain.MaximumGrainsPerCob.FixedValue'].max()\n\nGenotypes.loc[mask, ['File', 'Genotype']]",
    "crumbs": [
      "Masking genotypes"
    ]
  },
  {
    "objectID": "02_parquet_dataloader.html#convert-small-datasets-to-tensors",
    "href": "02_parquet_dataloader.html#convert-small-datasets-to-tensors",
    "title": "apsimxml",
    "section": "Convert Small Datasets to Tensors",
    "text": "Convert Small Datasets to Tensors\n\nGenotypes (Cultivar variables)\nWarning! There are some NAs from Genotypes that are not “Cultivar” Genotypes. These are calibrated genotypes that with defaults that are not clear.\n\n# keep as df or matrix (contains text)\nGenotypes_lookup = Genotypes.loc[:, ['File', 'Genotype']].copy()\n\n# ['Grain.MaximumGrainsPerCob.FixedValue',\n#  'Grain.MaximumPotentialGrainSize.FixedValue',\n#  'Phenology.FlagLeafToFlowering.Target.FixedValue',\n#  'Phenology.FloweringToGrainFilling.Target.FixedValue',\n#  'Phenology.GrainFilling.Target.FixedValue',\n#  'Phenology.Juvenile.Target.FixedValue',\n#  'Phenology.Maturing.Target.FixedValue',\n#  'Phenology.MaturityToHarvestRipe.Target.FixedValue',\n#  'Phenology.Photosensitive.Target.XYPairs.X__1',\n#  'Phenology.Photosensitive.Target.XYPairs.X__2',\n#  'Phenology.Photosensitive.Target.XYPairs.X__3',\n#  'Phenology.Photosensitive.Target.XYPairs.Y__1',\n#  'Phenology.Photosensitive.Target.XYPairs.Y__2',\n#  'Phenology.Photosensitive.Target.XYPairs.Y__3',\n#  'Rachis.DMDemands.Structural.DMDemandFunction.MaximumOrganWt.FixedValue']\n\nGenotypes_cols = list(Genotypes)\nGenotypes = Genotypes.drop(columns=['File', 'Genotype'])\n# coerce None to NaN so we can convert to matrix\nfor e in list(Genotypes):\n    Genotypes[e] = Genotypes[e].astype(float)\n\n# Genotypes = torch.tensor(Genotypes.to_numpy())\n\n\nGenotypes_lookup\n\n\n# for a given idx and year...\nidx_Ids = 1\nyear = 2000\n\n\n# get lookup information\nlookup = Ids.loc[idx_Ids, ].to_dict()\n\n\nmask = ((Genotypes_lookup.File == lookup['File']) &  (Genotypes_lookup.Genotype == lookup['Genotype']))\n# should only have a single value\nassert sum(mask) == 1\n\nidx_Geno = Genotypes_lookup.loc[mask, ].index[0]\n\nGenotypes[idx_Geno]\n\n\nlookup\n\n\nlookup_date = _prep_unix_epoch_to_date(max_year = 2024)\nlookup_date.head()\n\n\nResult = pq.read_table(apsimx_sim_parquet_dir+'/'+'sim_1698440407_4739.parquet').to_pandas()\n\n\n# lookup_* is a internally generated ref\n# *_lookup is a table based on loaded apsimx data\n\n\nResult_lookup = Result.loc[:, ['Date', 'FactorialUID']].copy()\nResult.drop(columns=['Date', 'FactorialUID'])\n\nResult_list = list(Result)\nResult = torch.tensor(Result.to_numpy())\n\nResult_lookup.head()\n\n\nprint(Result_lookup.shape)\nResult_lookup.merge(lookup_date.rename(columns={'Unix':'Date'}), how = 'left')\n\n\nResult_lookup.loc[(Result_lookup.FactorialUID == 24024), ].merge(lookup_date.rename(columns={'Unix':'Date'}), how = 'left').Date.max()\n\n\n((19250-5285)/365)+1984\n\n\nGenotypes_cols\n\n\nlookup\n\n\n# TODO make this valid for torch\nsow_date = lookup_date.loc[((lookup_date.Year == year) & \n                            (lookup_date.SowDate == lookup['SowDate'])), 'Unix'].values[0]\n\n# because of how this is set up index is also valid\n_ = lookup_date.loc[(lookup_date.Year == year), 'Unix'].agg(['min', 'max'])\n_['min']\n\n\nmask = (\n    (Result_lookup['Date'] &gt;= _['min']) & \n    (Result_lookup['Date'] &lt;= _['max']) &\n    (Result_lookup['FactorialUID'] == lookup['FactorialUID'])\n    )\n\nidx_Result = Result_lookup.loc[mask, ].index\n\nResult[idx_Result, ].shape\n\n\n#TODO make sure there aren't any values before the SowDate\nResult_list\n\n\nstart = 129486\n\n\n\npx.imshow((Result[(start-30):(start+30), 1:-1].numpy()[0:30, ]).transpose())\n\n\nimport matplotlib.pyplot as plt\n\npx.imshow(Result[idx_Result, ].numpy()[0:30, ])\n\n\nout = torch.zeros((365, 6))\n\nResult_lookup.loc[mask, ['Date']].min().values[0] - _['min']\n\n# sow_date - _['min']\n\n\ndatetime.datetime(1970, 1, 1, 0, 0) + datetime.timedelta(10957)\n\n\n# idx_Result.to_list()\n# Result\n\n\nGenotypes_cols",
    "crumbs": [
      "Masking genotypes"
    ]
  }
]