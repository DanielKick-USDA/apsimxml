# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_parquet_dataloader.ipynb.

# %% auto 0
__all__ = ['data_helper', 'apsimxDataset']

# %% ../nbs/02_parquet_dataloader.ipynb 2
import os
import numpy as np
import pandas as pd
import einops
import torch 

from torch.utils.data import Dataset
from torch.utils.data import DataLoader

from tqdm import tqdm

import pyarrow as pa
import pyarrow.parquet as pq

import datetime

# %% ../nbs/02_parquet_dataloader.ipynb 3
def _prep_unix_epoch_to_date(max_year:int = 2025 # Return dates from 1970 to `max_year`. 
                             ):
    "Provide unix epoch to date conversion table from 1970 to `max_year`."
    month_abbr = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
    max_day = (datetime.datetime(max_year, 1, 1, 0, 0) - datetime.datetime(1970, 1, 1, 0, 0)).days 
    unix_epoch = [i for i in range(max_day)]
    date_times = [datetime.datetime(1970, 1, 1, 0, 0) + datetime.timedelta(i) for i in range(max_day)]

    def as_SowDate(date_time):
        day = f'{date_time.day}'
        if len(day) == 1:
            return(f'0{day}-{month_abbr[date_time.month - 1]}')
        else:
            return(f'{day}-{month_abbr[date_time.month - 1]}')

    tmp = pd.DataFrame({
        'Unix': unix_epoch,
        'Year':[e.year for e in date_times],
        'Month':[e.month for e in date_times],
        'Day':[e.day for e in date_times],
        'SowDate':[as_SowDate(date_time = e) for e in date_times]
    })
    _ = tmp.loc[:, ['Unix', 'Year']].groupby(['Year']).min().reset_index().rename(columns={'Unix':'MinUnix'})
    tmp = tmp.merge(_)

    tmp['DOY'] = tmp['Unix'] - tmp['MinUnix']
    tmp = tmp.drop(columns = ['MinUnix'])
    return tmp

# %% ../nbs/02_parquet_dataloader.ipynb 8
class data_helper():
    "Abstract away the work finding, filtering, and loading the needed data."
    def __init__(self, 
                 genotypes_path:str, # Path to the file containing the simulated cultivar parameters table "Genotypes.parquet"
                 ids_path:str, # Path to the file containing the identifiers table that connects results with simulation settings "Ids.parquet"
                 results_path:str, # Path to the _directory_ containg simulation results tables as parquet files.
                 ssurgo_path:str, # Path to the file containing soil profiles "apsimx_i_soil_ssurgo_profile.parquet"
                 met_path:str, # Path to the file containing weather data "apsimx_i_soil_POWER_met.parquet"
                 ):
        # used later to get the results. Append / if there isn't one.
        if results_path[-1] != '/': 
            results_path = results_path+'/'
        self.results_path = results_path

        Genotypes = pq.read_table(genotypes_path).to_pandas()
        # coerce None to NaN (default cultivars don't have all values specified)
        for e in [ee for ee in list(Genotypes) if ee not in ['File', 'Genotype']]:
            Genotypes[e] = Genotypes[e].astype(float)
        mask = (Genotypes.isna().sum(axis = 1) == 0)

        self.Genotypes = Genotypes.loc[mask, ].reset_index(drop = True)
        self.Ids = pq.read_table(ids_path).to_pandas()

        # Environmental data
        self.ssurgo_data = pq.read_table(ssurgo_path
                            ).to_pandas(
                            ).rename(columns = {'soil_i':'SoilIdx'})
        
        self.met_data    = pq.read_table(met_path
                            ).to_pandas(
                            ).rename(columns = {'latitude':  'Latitude',
                                                'longitude': 'Longitude'})

    def apply_mask(self, 
                   table:str, # Name of the table to be filtered. Either "Genotypes" or "Ids"
                   mask:pd.DataFrame # Mask to be applied. 
                   ):
        "Filter a table with a mask then automatically filtering the non-masked table to ensure the keys stay in sync."
        if table not in ['Genotypes', 'Ids']:
            print('table should be Genotypes or Ids')   
        else:
            if table == 'Genotypes':
                # apply mask to filter the table
                self.Genotypes = self.Genotypes.loc[mask, ].reset_index(drop = True)
                # left join to update the other table
                self.Ids = self.Genotypes.loc[:, ['File', 'Genotype']
                                        ].drop_duplicates(
                                        ).merge(self.Ids, how = 'left'
                                        ).reset_index(drop = True)
            elif table == 'Ids':
                self.Ids = self.Ids.loc[mask, ].reset_index(drop = True)
                self.Genotypes = self.Ids.loc[:, ['File', 'Genotype']
                                    ].drop_duplicates(
                                    ).merge(self.Genotypes, how = 'left'
                                    ).reset_index(drop = True)
                
    def load_results(self, 
                     years:list = [], # List of years to consider data from. An empty list will return all years available 
                     dry_run:bool = True # Should files to be read be loaded in or only identified? (Reading may take a while.)
                     ):
        "Identify and load all results for for the records in the Ids and Genotypes tables. Optionally filter to specific years."
        self.years = years

        # Now we can ask for the files that we should get
        tmp = self.Ids.loc[:, ['File', 'Genotype', 'FactorialUID']]

        get_files = tmp.File.drop_duplicates().to_list()
        print(f'{len(get_files)} files to be read.')

        if dry_run == True:
            print('In dry_run, reading no files')

        if dry_run == False:
            res_list = []
            col_order = ''
            for file in tqdm(get_files):
                # print(f'{file}')
                res = pq.read_table(f'{x.results_path+file}.parquet').to_pandas()

                # columns should be in the same order, but we will force them to be here. 
                if col_order == '':
                    col_order = list(res)

                # filter order established with some informal testing.
                # runtime filter years, factorials: [10, 11.3, 10.5]
                # runtime filter factorials, years: [9.0, 8.4, 9.6]

                # filter factorials
                res = tmp.loc[(tmp.File == file), ['FactorialUID']].merge(res)

                # filter years
                if years != []:
                    yr = _prep_unix_epoch_to_date(max_year = 2030)
                    yr = yr.loc[(yr.Year.isin(years)), ['Unix']].rename(columns={'Unix':'Date'})
                    res = yr.merge(res)

                res_list.append(res)
            res_list = pd.concat(res_list).reset_index(drop=True)
            self.results = res_list

    def setup_encoders(self):
        "Create simple mappings of strings to ints so that so we can use tensors for all lookups."
        Ids = self.Ids
        self.encoder_Genotype = {k:v for k,v in zip(Ids.Genotype.unique(), range(len(Ids.Genotype.unique())))}
        self.encoder_File     = {k:v for k,v in zip(Ids.File.unique(),     range(len(Ids.File.unique())))}
        self.encoder_SowDate  = {k:v for k,v in zip(Ids.SowDate.unique(),  range(len(Ids.SowDate.unique())))}

    def results_as_arrays(self, 
                          output_type:str = 'point' # Type of phenotype output, either a point estimate (max value) or a time series. Take either "point" or "sequence".
                          )->tuple: 
        """
        Returns a tuple countaining a lookup array, results array, and reference dictionary. 
        The lookup contains columns to map a result to the Ids table's values. 
        The results array contains either point estimates or a sequence for some observed metrics.
        The reference dictionary contains the names of the columns in the lookup and results arrays.
        """
        metadata = ['FactorialUID', 'Year']
        metrics  = ['Maize.AboveGround.Wt', 'Maize.LAI', 'yield_Kgha']
        ref = {
            'lookup_names': metadata,
            'data_names': metrics,
        }

        # Turn results into tensor
        tmp = self.results.merge(_prep_unix_epoch_to_date(max_year = 2025).loc[:, ['Unix', 'Year', 'DOY']].rename(columns={'Unix':'Date'}))

        # find the smallest above ground weight within each year, usethat to get the first date within eacy year and then filter for the values that are >= the within year start.
        _ = tmp.loc[:, metadata+['Maize.AboveGround.Wt']].groupby(metadata).agg('min').reset_index()
        _ = _.merge(tmp).loc[:, metadata+['Date']].rename(columns={'Date': 'YearStart'})
        tmp = tmp.merge(_, how='left')

        tmp = tmp.loc[(tmp.Date >= tmp.YearStart), ]

        tmp=tmp.drop(columns=['YearStart', 'Date']
            ).drop_duplicates(
            ).sort_values(metadata+['DOY']
            ).reset_index(drop = True)

        if output_type not in ['point', 'sequence']:
            print('output_type expected to be point or sequence')
        if output_type == 'point':
            ref['data_dims'] = ['obs', 'metrics']
            # if making point estimates:
            tmp = tmp.drop(columns='DOY').groupby(metadata).agg('max').reset_index()
            #TODO remove the conversion to tensor?
            lookup = torch.tensor(tmp.loc[:, metadata].to_numpy())
            out = torch.tensor(tmp.drop(columns=metadata).to_numpy())
        
        if output_type == 'sequence':
            ref['data_dims'] = ['obs', 'days', 'metrics']
            day_lookup = _prep_unix_epoch_to_date(max_year = 2025).rename(columns={'Unix':'Date'})
            # day_lookup
            lookup = tmp.loc[:, metadata].drop_duplicates().reset_index(drop = True)
            
            # obs, channels, length
            out = torch.zeros((lookup.shape[0], 
                            365,
                            3
                            ))
            
            for i in tqdm(range(lookup.shape[0])):
                # break
                uid, yr = lookup.loc[i, ]

                # use Ids table to get planting date. 
                sow_date = self.Ids.loc[(self.Ids.FactorialUID == uid), 'SowDate']

                plant_DOY = day_lookup.loc[(
                    (day_lookup.Year == yr) & 
                    (day_lookup.SowDate == sow_date.values[0])), 'DOY'].values[0]


                mask = (
                    (tmp.FactorialUID == uid) & 
                    (tmp.Year == yr) &
                    (tmp.DOY >= plant_DOY) & 
                    (tmp.DOY <= 364) # because there are leap years and indexing starts at 0, clip to the minimum of these
                    )
                
                start_DOY= tmp.loc[mask, 'DOY'].min()
                stop_DOY = tmp.loc[mask, 'DOY'].max()

                out[i, start_DOY:(stop_DOY+1), :] = torch.tensor(tmp.loc[mask, metrics].to_numpy())
                # # propagate forward observations to all dates following the max
                out[i, stop_DOY:365, :] = out[i, stop_DOY, :]
            
        return(lookup, out, ref)  
    
    # Working with Environmental Data
    def filter_env(self):
        "Filter records to remove enviromental data that aren't needed, then refresh indices."
        # filter to selected years
        if self.years != []:
            self.met_data = self.met_data.loc[(self.met_data.year.isin(self.years)), ].reset_index(drop = True)

        # shared soilIdx
        Ids_idxs = self.Ids.loc[:, ['Longitude', 'Latitude', 'SoilIdx']].drop_duplicates()
        ssurgo_s = self.ssurgo_data.loc[:, ['SoilIdx']].drop_duplicates()
        met_lonlat = self.met_data.loc[:, ['Longitude', 'Latitude']].drop_duplicates()

        print(f'Uniq. Lon/Lat/Soil: {Ids_idxs.shape[0]}')
        print('Condition on env availability:')
        Ids_idxs = Ids_idxs.merge(ssurgo_s, how = 'inner').merge(met_lonlat, how = 'inner')
        print(f'Uniq. Lon/Lat/Soil: {Ids_idxs.shape[0]}')

        # reduce the environmental datasets:
        self.met_data    = Ids_idxs.drop(columns=['SoilIdx']).drop_duplicates().merge(self.met_data, how = 'inner')
        self.ssurgo_data = Ids_idxs.drop(columns=['Longitude', 'Latitude']).drop_duplicates().merge(self.ssurgo_data, how = 'inner')

        # update Ids & results:
        self.Ids = Ids_idxs.merge(self.Ids)
        _ = self.Ids.loc[:, ['FactorialUID']].drop_duplicates()
        self.results = _.merge(self.results, how='inner')

        # refresh index on everything
        self.Ids         = self.Ids.reset_index(        drop = True)
        self.results     = self.results.reset_index(    drop = True)
        self.Genotypes   = self.Genotypes.reset_index(  drop = True)
        self.met_data    = self.met_data.reset_index(   drop = True)
        self.ssurgo_data = self.ssurgo_data.reset_index(drop = True)

    def met_as_arrays(self, 
                      ops_string:str = 's d m -> s d m' # Operations string for einops.rearranges. Allows for axis to be swaped. At present order of "site, day, metric" is not swapped.
                      )->tuple:
        """
        Returns a tuple countaining a lookup array, data array, and reference dictionary containing the column names of the lookup and data arrays and the names of the dimensions of data. 
        """
        m = self.met_data.copy()
        m = m.rename(columns = {'year': 'Year'})
        metadata = [
            'Longitude', 
            'Latitude', 
            'Year'
            ]
        metrics = [
            'radn_MJ_div_m2_div_day',
            'maxt_oC',
            'mint_oC',
            'rain_mm',
            'rh_pr',
            'windspeed_m_div_s',
            'tav',
            'amp'
            ]


        # make sure that there are the expected number of values per group
        m = m.loc[(m.day < 366), ].sort_values(metadata).reset_index(drop = True)
        # there's at least one site that appears to have at least two records. I'll deal with this by (tav and amp appear to be slightly different)
        # see m.loc[((m.Longitude == -86.529600) & (m.Latitude == 34.729520) & (m.year == 2014) ), ].drop_duplicates().sort_values('day')

        m = m.groupby(metadata+['day']).mean().reset_index()

        # all groups have 365 obs?
        assert False not in (m.groupby(metadata).count().reset_index().loc[:, ['day']] == 365).values

        lookup = m.loc[:, metadata].drop_duplicates().to_numpy()

        m = m.loc[:, metrics
            ].to_numpy(
            ).reshape(
                -1,           # as many sites as are available
                365,          # days
                len(metrics)) # metrics


        m = einops.rearrange(m, ops_string)

        # let's also allow for an operations string so that we can _request_ data in a specific format instead of changing it after the fact.
        ops = ops_string.replace(' ', '').split('->')
        dim_order = [i
        for e in ops[1]
        for i in range(len(ops[0]))
        if e == ops[0][i]
        ]

        ref = {
            'lookup_names': metadata,
            'data_names': metrics,
            'data_dims': [['site', 'days', 'metrics'][i] for i in dim_order] # use the ops string to figure out the order that will be returned
        }

        return (lookup, m, ref)

    def ssurgo_as_arrays(self, 
                         ops_string = 's l m -> s l m' # Operations string for einops.rearranges. Allows for axis to be swaped. At present order of "site, length (depth), metric" is not swapped.
                         )->tuple:              
        """
        Returns a tuple countaining a lookup array, data array, and reference dictionary containing the column names of the lookup and data arrays and the names of the dimensions of data. 
        """
        s = self.ssurgo_data.copy()

        metadata = [
            'SoilIdx',
            ]
        metrics = [
            'BD',
            'AirDry',
            'LL15',
            'DUL',
            'SAT',
            'KS',
            'Carbon',
            'SoilCNRatio',
            'FOM',
            'FOM.CN',
            'FBiom',
            'FInert',
            'NO3N',
            'NH4N',
            'PH',
            'ParticleSizeClay',
            'ParticleSizeSilt',
            'ParticleSizeSand',
            'Maize.KL',
            'Maize.LL',
            'Maize.XF',
            # 'Soybean.KL',
            # 'Soybean.LL',
            # 'Soybean.XF',
            # 'Wheat.KL',
            # 'Wheat.LL',
            # 'Wheat.XF'
            ]

        # check that the soil slices are equally sized
        assert len(s.Thickness.unique()) == 1
        print(f'Soil compartment thickness: {s.Thickness[0]}')

        # check that there are an equal number of slices
        _  = s.loc[:, ['SoilIdx', 'Thickness']].groupby(['SoilIdx']).count().reset_index()
        assert _.Thickness.min() == _.Thickness.max()

        # turn range into start of slice
        s['Depth'] = s['Depth'].str.split(pat = '-', expand = True)[0].astype(int)
        s = s.sort_values(['SoilIdx', 'Depth']).reset_index(drop = True)

        lookup = s.loc[:, metadata].drop_duplicates().to_numpy()

        s = s.loc[:, metrics].to_numpy()
        s = s.reshape(-1, lookup.shape[0], len(metrics))


        s = einops.rearrange(s, ops_string)

        # let's also allow for an operations string so that we can _request_ data in a specific format instead of changing it after the fact.
        ops = ops_string.replace(' ', '').split('->')
        dim_order = [i
        for e in ops[1]
        for i in range(len(ops[0]))
        if e == ops[0][i]
        ]


        ref = {
            'lookup_names': metadata,
            'data_names': metrics,
            'data_dims': [['site', 'depth', 'metrics'][i] for i in dim_order] # use the ops string to figure out the order that will be returned
        }
        return (lookup, s, ref)
    
    def genotype_as_arrays(self)->tuple:
        """
        Returns a tuple countaining a lookup array, data array, and reference dictionary containing the column names of the lookup and data arrays and the names of the dimensions of data. 
        """
        g = self.Genotypes.copy()

        # Genotypes are easy to deal with because the table is cleaned up in filtering results. 
        metadata = [
            'File',
            'Genotype',
            ]
        metrics = [
            'Grain.MaximumGrainsPerCob.FixedValue',
            'Grain.MaximumPotentialGrainSize.FixedValue',
            'Phenology.FlagLeafToFlowering.Target.FixedValue',
            'Phenology.FloweringToGrainFilling.Target.FixedValue',
            'Phenology.GrainFilling.Target.FixedValue',
            'Phenology.Juvenile.Target.FixedValue',
            'Phenology.Maturing.Target.FixedValue',
            'Phenology.MaturityToHarvestRipe.Target.FixedValue',
            'Phenology.Photosensitive.Target.XYPairs.X__1',
            'Phenology.Photosensitive.Target.XYPairs.X__2',
            'Phenology.Photosensitive.Target.XYPairs.X__3',
            'Phenology.Photosensitive.Target.XYPairs.Y__1',
            'Phenology.Photosensitive.Target.XYPairs.Y__2',
            'Phenology.Photosensitive.Target.XYPairs.Y__3',
            'Rachis.DMDemands.Structural.DMDemandFunction.MaximumOrganWt.FixedValue',
        ]

        lookup = g.loc[:, metadata]
        lookup.File     = [self.encoder_File[e]     for e in lookup.File]
        lookup.Genotype = [self.encoder_Genotype[e] for e in lookup.Genotype]
        lookup = lookup.to_numpy()

        g = g.loc[:, metrics].to_numpy()

        ref = {
            'lookup_names': metadata,
            'data_names': metrics,
            'data_dims': ('genotype', 'metrics')
        }
        return(lookup, g, ref)
    
    def ids_as_arrays(self)->tuple:
        """
        Apply the simple int encoder to the Ids table then return a tuple countaining a lookup array and reference dictionary containing the column names of the lookup. 
        """
        Ids =  self.Ids.copy()
        # apply encoder
        Ids.Genotype = [self.encoder_Genotype[e] for e in Ids.Genotype]
        Ids.File     = [self.encoder_File[e]     for e in Ids.File]
        Ids.SowDate  = [self.encoder_SowDate[e]  for e in Ids.SowDate]

        ref = {'lookup_names': list(Ids),}
        Ids = Ids.to_numpy()
        return(Ids, ref)
    
    def sowing_as_array(self)->tuple:
        "Precompute a lookup table to convert Year/SowDate to DOY. Return a lookup array and reference dictionary."

        # If .years is an empty list, make one on the fly.
        if self.years != []:
            years = self.years
        else:
            years = self.results.loc[:, ['Date']].rename(columns={'Date':'Unix'}
                       ).merge(_prep_unix_epoch_to_date(max_year = 2025).loc[:, ['Unix','Year']]
                       ).loc[:, 'Year'].drop_duplicates().to_list()

        _ = pd.DataFrame([(y, s) for y in years for s in self.encoder_SowDate], columns=['Year', 'SowDate'])
        # Add in DOY
        _=_.merge(_prep_unix_epoch_to_date(max_year = 2025).loc[:, ['Year', 'SowDate', 'DOY']])
        # overwrite SowDate with encoded version

        _.SowDate = [x.encoder_SowDate[e] for e in _.SowDate]

        ref = {
            'lookup_names': list(_)
        }
        planting_lookup = _.to_numpy()
        
        return(planting_lookup, ref)

# %% ../nbs/02_parquet_dataloader.ipynb 12
class apsimxDataset(Dataset):
    "PyTorch dataset to provide simulated phenotype and enviromental covariates. Currently relies on arrays produced by `data_helper`. `__getitem__` returns (y, cultivar, ssurgo, met)"
    def __init__(
            self, 
            result_lookup:torch.tensor,   # Simulated Phenotype Lookup   
            result:torch.tensor,          # Simulated Phenotype Data
            ids_lookup:torch.tensor,      # Identifyer Lookup
            sow_lookup:torch.tensor,      # Encoded SowDate to DOY Lookup 
            sow_met_concat:bool,          # Specifies wheter the weather tensor should have a 0/1 vector specifying whether the crop has been planted 
            genotype_lookup:torch.tensor, # Cultivar Parameter Lookup
            genotype:torch.tensor,        # Cultivar Parameter Data
            met_lookup:torch.tensor,      # Weather Lookup     
            met:torch.tensor,             # Weather Data
            ssurgo_lookup:torch.tensor,   # Soil Lookup  
            ssurgo:torch.tensor,          # Soil Data
            ):
        # super().__init__()
        self.result_lookup = result_lookup
        self.result = result
        self.ids_lookup = ids_lookup
        self.sow_lookup = sow_lookup
        self.sow_met_concat = sow_met_concat
        self.genotype_lookup = genotype_lookup
        self.genotype = genotype
        self.met_lookup = met_lookup
        self.met = met
        self.ssurgo_lookup = ssurgo_lookup
        self.ssurgo = ssurgo

    def __len__(self):
        return len(self.result_lookup.shape[0])
    
    def __getitem__(self, idx):
        # setup keys
        # get keys from y
        FactorialUID, Year = self.result_lookup[idx, ]
        # get use ids to get env keys from Ids table
        Longitude, Latitude, SoilIdx, File, Genotype, SowDate, FactorialUID = self.ids_lookup[(self.ids_lookup[:, 6] == int(FactorialUID)), ].squeeze()
        # now we have the full set of keys.

        # get output values
        y = self.result[idx]

        # cultivar variables
        mask = (
            (self.genotype_lookup[:, 0] == File) & 
            (self.genotype_lookup[:, 1] == Genotype)
            )
        cultivar = self.genotype[mask, ]

        # ssurgo data
        mask = (self.ssurgo_lookup[:, 0] == int(SoilIdx))
        ssurgo = self.ssurgo[:, mask, :]

        # met data
        mask = (
            (self.met_lookup[:, 0] == Longitude) & 
            (self.met_lookup[:, 1] == Latitude) & 
            (self.met_lookup[:, 2] == float(Year)) 
            )
        met = self.met[mask, :, :]
        if self.sow_met_concat:
            # Concatenate a column with 0/1 for whether sowing has occured.
            mask = (
                (self.sow_lookup[:, 0] == Year) &
                (self.sow_lookup[:, 1] == SowDate)
                )
            sow_doy = self.sow_lookup[mask, 2]
            _ = torch.zeros([365])
            _[sow_doy:] = 1
            met = torch.concat([met, _[None, :, None]], axis = 2)

        return (y, cultivar, ssurgo, met)
