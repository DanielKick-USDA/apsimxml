{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import einops\n",
    "import torch \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# Compute unix epoch to date table\n",
    "\n",
    "def _prep_unix_epoch_to_date(max_year = 2025):\n",
    "    month_abbr = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "    max_day = (datetime.datetime(max_year, 1, 1, 0, 0) - datetime.datetime(1970, 1, 1, 0, 0)).days \n",
    "    unix_epoch = [i for i in range(max_day)]\n",
    "    date_times = [datetime.datetime(1970, 1, 1, 0, 0) + datetime.timedelta(i) for i in range(max_day)]\n",
    "\n",
    "    def as_SowDate(date_time):\n",
    "        day = f'{date_time.day}'\n",
    "        if len(day) == 1:\n",
    "            return(f'0{day}-{month_abbr[date_time.month - 1]}')\n",
    "        else:\n",
    "            return(f'{day}-{month_abbr[date_time.month - 1]}')\n",
    "\n",
    "    tmp = pd.DataFrame({\n",
    "        'Unix': unix_epoch,\n",
    "        'Year':[e.year for e in date_times],\n",
    "        'Month':[e.month for e in date_times],\n",
    "        'Day':[e.day for e in date_times],\n",
    "        'SowDate':[as_SowDate(date_time = e) for e in date_times]\n",
    "    })\n",
    "    _ = tmp.loc[:, ['Unix', 'Year']].groupby(['Year']).min().reset_index().rename(columns={'Unix':'MinUnix'})\n",
    "    tmp = tmp.merge(_)\n",
    "\n",
    "    tmp['DOY'] = tmp['Unix'] - tmp['MinUnix']\n",
    "    tmp = tmp.drop(columns = ['MinUnix'])\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how bad would it be if we stored a bunch of tiny parquet files? \n",
    "# saving many tiny parquet files will increase storage cost by 1.6x\n",
    "# That's not great but not horrible either. We're talking about approximately 500 gb.\n",
    "# (305213583096*1.6)/1000000000\n",
    "# 488.34\n",
    "\n",
    "# apsimx_sim_parquet_dir = '/home/Shared/cultivar_sim_exps'\n",
    "# Result = pq.read_table(apsimx_sim_parquet_dir+'/'+'sim_1698440407_4739.parquet').to_pandas()\n",
    "\n",
    "# -rw-rw-r-- 1 kickd newgroup 2939581425 Jun  1 01:08 cultivar_sim_exps/sim_1698440407_4739.parquet\n",
    "\n",
    "\n",
    "# for i in tqdm(Result.FactorialUID.unique()):\n",
    "#     table = Result.loc[(Result.FactorialUID == i), ].drop(columns = 'FactorialUID')\n",
    "#     table = pa.Table.from_pandas(table)\n",
    "#     pq.write_table(table, f'/home/Shared/testing_rm_after0620/{i}.parquet')\n",
    "\n",
    "# 100%|██████████| 24025/24025 [33:58<00:00, 11.79it/s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The goal is to have a (postgres?) SQL db that we can query. To not have a delay we're going to instead load a batch into memory BUT allow for redefining this batch by swapping out the dataloader.\n",
    "\n",
    "# Workflow:\n",
    "# Define the desired data \n",
    "# Use the main tables to figure out what parquet files we need to pull from.\n",
    "# Pull all the data in and represent as tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sim_1698440407_4739.parquet',\n",
       " 'sim_1697418008_10643.parquet',\n",
       " 'sim_1697187607_79518.parquet']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apsimx_sim_parquet_dir = '/home/Shared/cultivar_sim_exps'\n",
    "os.listdir(apsimx_sim_parquet_dir)[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DefaultCultivarsAll.parquet', 'Genotypes.parquet', 'Ids.parquet']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e for e in os.listdir(apsimx_sim_parquet_dir) if e[0:4] != 'sim_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata \n",
    "DefaultCultivarsAll = pq.read_table(apsimx_sim_parquet_dir+'/'+'DefaultCultivarsAll.parquet').to_pandas()\n",
    "Genotypes           = pq.read_table(apsimx_sim_parquet_dir+'/'+'Genotypes.parquet').to_pandas()\n",
    "Ids                 = pq.read_table(apsimx_sim_parquet_dir+'/'+'Ids.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Mask: Genotypes: (3150, 17) Ids: (2837275, 7)\n",
      "After Mask: Genotypes: (5, 17) Ids: (4247, 7)\n",
      "5 files to be read.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:25<00:00,  5.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uniq. Lon/Lat/Soil: 126\n",
      "Condition on env availability:\n",
      "Uniq. Lon/Lat/Soil: 109\n"
     ]
    }
   ],
   "source": [
    "# I'm setting up a class to help find the files we need to read through to build the datset \n",
    "# This works by holding a copy of the Ids and Genotypes tables. \n",
    "# We'll operate on those, filtering them down until the tables only contain the enries we want to use.\n",
    "# Next we'll use a that produces tuples of the (parquet file, filtering criteria)\n",
    "\n",
    "class data_helper():\n",
    "    def __init__(self, genotypes_path, ids_path, results_path, ssurgo_path, met_path):\n",
    "        # used later to get the results. Append / if there isn't one.\n",
    "        if results_path[-1] != '/': \n",
    "            results_path = results_path+'/'\n",
    "        self.results_path = results_path\n",
    "\n",
    "        Genotypes = pq.read_table(genotypes_path).to_pandas()\n",
    "        # coerce None to NaN (default cultivars don't have all values specified)\n",
    "        for e in [ee for ee in list(Genotypes) if ee not in ['File', 'Genotype']]:\n",
    "            Genotypes[e] = Genotypes[e].astype(float)\n",
    "        mask = (Genotypes.isna().sum(axis = 1) == 0)\n",
    "\n",
    "        self.Genotypes = Genotypes.loc[mask, ].reset_index(drop = True)\n",
    "        self.Ids = pq.read_table(ids_path).to_pandas()\n",
    "\n",
    "        # Environmental data\n",
    "        self.ssurgo_data = pq.read_table(ssurgo_path\n",
    "                            ).to_pandas(\n",
    "                            ).rename(columns = {'soil_i':'SoilIdx'})\n",
    "        \n",
    "        self.met_data    = pq.read_table(met_path\n",
    "                            ).to_pandas(\n",
    "                            ).rename(columns = {'latitude':  'Latitude',\n",
    "                                                'longitude': 'Longitude'})\n",
    "\n",
    "    \n",
    "    def apply_mask(self, table, mask):\n",
    "        \"This method takes care of automatically filtering the non-masked table.\"\n",
    "        if table not in ['Genotypes', 'Ids']:\n",
    "            print('table should be Genotypes or Ids')   \n",
    "        else:\n",
    "            if table == 'Genotypes':\n",
    "                # apply mask to filter the table\n",
    "                self.Genotypes = self.Genotypes.loc[mask, ].reset_index(drop = True)\n",
    "                # left join to update the other table\n",
    "                self.Ids = self.Genotypes.loc[:, ['File', 'Genotype']\n",
    "                                        ].drop_duplicates(\n",
    "                                        ).merge(self.Ids, how = 'left'\n",
    "                                        ).reset_index(drop = True)\n",
    "            elif table == 'Ids':\n",
    "                self.Ids = self.Ids.loc[mask, ].reset_index(drop = True)\n",
    "                self.Genotypes = self.Ids.loc[:, ['File', 'Genotype']\n",
    "                                    ].drop_duplicates(\n",
    "                                    ).merge(self.Genotypes, how = 'left'\n",
    "                                    ).reset_index(drop = True)\n",
    "    def load_results(self, years = [], dry_run = True):\n",
    "        self.years = years\n",
    "        # Now we can ask for the files that we should get\n",
    "        tmp = self.Ids.loc[:, ['File', 'Genotype', 'FactorialUID']]\n",
    "\n",
    "        get_files = tmp.File.drop_duplicates().to_list()\n",
    "        print(f'{len(get_files)} files to be read.')\n",
    "\n",
    "        if dry_run == True:\n",
    "            print('In dry_run, reading no files')\n",
    "\n",
    "        if dry_run == False:\n",
    "            res_list = []\n",
    "            col_order = ''\n",
    "            for file in tqdm(get_files):\n",
    "                # print(f'{file}')\n",
    "                res = pq.read_table(f'{x.results_path+file}.parquet').to_pandas()\n",
    "\n",
    "                # columns should be in the same order, but we will force them to be here. \n",
    "                if col_order == '':\n",
    "                    col_order = list(res)\n",
    "\n",
    "                # filter order established with some informal testing.\n",
    "                # runtime filter years, factorials: [10, 11.3, 10.5]\n",
    "                # runtime filter factorials, years: [9.0, 8.4, 9.6]\n",
    "\n",
    "                # filter factorials\n",
    "                res = tmp.loc[(tmp.File == file), ['FactorialUID']].merge(res)\n",
    "\n",
    "                # filter years\n",
    "                if years != []:\n",
    "                    yr = _prep_unix_epoch_to_date(max_year = 2030)\n",
    "                    yr = yr.loc[(yr.Year.isin(years)), ['Unix']].rename(columns={'Unix':'Date'})\n",
    "                    res = yr.merge(res)\n",
    "\n",
    "                res_list.append(res)\n",
    "            res_list = pd.concat(res_list).reset_index(drop=True)\n",
    "            self.results = res_list\n",
    "\n",
    "    def setup_encoders(self):\n",
    "        # create string to num encoders so we can use tensors for all lookups\n",
    "        Ids = self.Ids\n",
    "        self.encoder_Genotype = {k:v for k,v in zip(Ids.Genotype.unique(), range(len(Ids.Genotype.unique())))}\n",
    "        self.encoder_File     = {k:v for k,v in zip(Ids.File.unique(),     range(len(Ids.File.unique())))}\n",
    "        self.encoder_SowDate  = {k:v for k,v in zip(Ids.SowDate.unique(),  range(len(Ids.SowDate.unique())))}\n",
    "\n",
    "    def results_as_arrays(self, output_type = 'point'): #output_type = 'sequence'\n",
    "        metadata = ['FactorialUID', 'Year']\n",
    "        metrics  = ['Maize.AboveGround.Wt', 'Maize.LAI', 'yield_Kgha']\n",
    "        ref = {\n",
    "            'lookup_names': metadata,\n",
    "            'data_names': metrics,\n",
    "        }\n",
    "\n",
    "        # Turn results into tensor\n",
    "        tmp = self.results.merge(_prep_unix_epoch_to_date(max_year = 2025).loc[:, ['Unix', 'Year', 'DOY']].rename(columns={'Unix':'Date'}))\n",
    "\n",
    "        # find the smallest above ground weight within each year, usethat to get the first date within eacy year and then filter for the values that are >= the within year start.\n",
    "        _ = tmp.loc[:, metadata+['Maize.AboveGround.Wt']].groupby(metadata).agg('min').reset_index()\n",
    "        _ = _.merge(tmp).loc[:, metadata+['Date']].rename(columns={'Date': 'YearStart'})\n",
    "        tmp = tmp.merge(_, how='left')\n",
    "\n",
    "        tmp = tmp.loc[(tmp.Date >= tmp.YearStart), ]\n",
    "\n",
    "        tmp=tmp.drop(columns=['YearStart', 'Date']\n",
    "            ).drop_duplicates(\n",
    "            ).sort_values(metadata+['DOY']\n",
    "            ).reset_index(drop = True)\n",
    "\n",
    "        if output_type not in ['point', 'sequence']:\n",
    "            print('output_type expected to be point or sequence')\n",
    "        if output_type == 'point':\n",
    "            ref['data_dims'] = ['obs', 'metrics']\n",
    "            # if making point estimates:\n",
    "            tmp = tmp.drop(columns='DOY').groupby(metadata).agg('max').reset_index()\n",
    "\n",
    "            lookup = torch.tensor(tmp.loc[:, metadata].to_numpy())\n",
    "            out = torch.tensor(tmp.drop(columns=metadata).to_numpy())\n",
    "        \n",
    "        if output_type == 'sequence':\n",
    "            ref['data_dims'] = ['obs', 'days', 'metrics']\n",
    "            day_lookup = _prep_unix_epoch_to_date(max_year = 2025).rename(columns={'Unix':'Date'})\n",
    "            # day_lookup\n",
    "            lookup = tmp.loc[:, metadata].drop_duplicates().reset_index(drop = True)\n",
    "            \n",
    "            # obs, channels, length\n",
    "            out = torch.zeros((lookup.shape[0], \n",
    "                            365,\n",
    "                            3\n",
    "                            ))\n",
    "            \n",
    "            for i in tqdm(range(lookup.shape[0])):\n",
    "                # break\n",
    "                uid, yr = lookup.loc[i, ]\n",
    "\n",
    "                # use Ids table to get planting date. \n",
    "                sow_date = self.Ids.loc[(self.Ids.FactorialUID == uid), 'SowDate']\n",
    "\n",
    "                plant_DOY = day_lookup.loc[(\n",
    "                    (day_lookup.Year == yr) & \n",
    "                    (day_lookup.SowDate == sow_date.values[0])), 'DOY'].values[0]\n",
    "\n",
    "\n",
    "                mask = (\n",
    "                    (tmp.FactorialUID == uid) & \n",
    "                    (tmp.Year == yr) &\n",
    "                    (tmp.DOY >= plant_DOY) & \n",
    "                    (tmp.DOY <= 364) # because there are leap years and indexing starts at 0, clip to the minimum of these\n",
    "                    )\n",
    "                \n",
    "                start_DOY= tmp.loc[mask, 'DOY'].min()\n",
    "                stop_DOY = tmp.loc[mask, 'DOY'].max()\n",
    "\n",
    "                out[i, start_DOY:(stop_DOY+1), :] = torch.tensor(tmp.loc[mask, metrics].to_numpy())\n",
    "                # # propagate forward observations to all dates following the max\n",
    "                out[i, stop_DOY:365, :] = out[i, stop_DOY, :]\n",
    "            \n",
    "        return(lookup, out, ref)  \n",
    "\n",
    "\n",
    "    # Working with Environmental Data\n",
    "    def filter_env(self):\n",
    "        # filter to selected years\n",
    "        if self.years != []:\n",
    "            self.met_data = self.met_data.loc[(self.met_data.year.isin(self.years)), ].reset_index(drop = True)\n",
    "\n",
    "        # shared soilIdx\n",
    "        Ids_idxs = self.Ids.loc[:, ['Longitude', 'Latitude', 'SoilIdx']].drop_duplicates()\n",
    "        ssurgo_s = self.ssurgo_data.loc[:, ['SoilIdx']].drop_duplicates()\n",
    "        met_lonlat = self.met_data.loc[:, ['Longitude', 'Latitude']].drop_duplicates()\n",
    "\n",
    "        print(f'Uniq. Lon/Lat/Soil: {Ids_idxs.shape[0]}')\n",
    "        print('Condition on env availability:')\n",
    "        Ids_idxs = Ids_idxs.merge(ssurgo_s, how = 'inner').merge(met_lonlat, how = 'inner')\n",
    "        print(f'Uniq. Lon/Lat/Soil: {Ids_idxs.shape[0]}')\n",
    "\n",
    "        # reduce the environmental datasets:\n",
    "        self.met_data    = Ids_idxs.drop(columns=['SoilIdx']).drop_duplicates().merge(self.met_data, how = 'inner')\n",
    "        self.ssurgo_data = Ids_idxs.drop(columns=['Longitude', 'Latitude']).drop_duplicates().merge(self.ssurgo_data, how = 'inner')\n",
    "\n",
    "        # update Ids & results:\n",
    "        self.Ids = Ids_idxs.merge(self.Ids)\n",
    "        _ = self.Ids.loc[:, ['FactorialUID']].drop_duplicates()\n",
    "        self.results = _.merge(self.results, how='inner')\n",
    "\n",
    "        # refresh index on everything\n",
    "        self.Ids         = self.Ids.reset_index(        drop = True)\n",
    "        self.results     = self.results.reset_index(    drop = True)\n",
    "        self.Genotypes   = self.Genotypes.reset_index(  drop = True)\n",
    "        self.met_data    = self.met_data.reset_index(   drop = True)\n",
    "        self.ssurgo_data = self.ssurgo_data.reset_index(drop = True)\n",
    "\n",
    "    def met_as_arrays(self, ops_string = 's d m -> s d m'): # site day metric\n",
    "        m = self.met_data.copy()\n",
    "        m = m.rename(columns = {'year': 'Year'})\n",
    "        metadata = [\n",
    "            'Longitude', \n",
    "            'Latitude', \n",
    "            'Year'\n",
    "            ]\n",
    "        metrics = [\n",
    "            'radn_MJ_div_m2_div_day',\n",
    "            'maxt_oC',\n",
    "            'mint_oC',\n",
    "            'rain_mm',\n",
    "            'rh_pr',\n",
    "            'windspeed_m_div_s',\n",
    "            'tav',\n",
    "            'amp'\n",
    "            ]\n",
    "\n",
    "\n",
    "        # make sure that there are the expected number of values per group\n",
    "        m = m.loc[(m.day < 366), ].sort_values(metadata).reset_index(drop = True)\n",
    "        # there's at least one site that appears to have at least two records. I'll deal with this by (tav and amp appear to be slightly different)\n",
    "        # see m.loc[((m.Longitude == -86.529600) & (m.Latitude == 34.729520) & (m.year == 2014) ), ].drop_duplicates().sort_values('day')\n",
    "\n",
    "        m = m.groupby(metadata+['day']).mean().reset_index()\n",
    "\n",
    "        # all groups have 365 obs?\n",
    "        assert False not in (m.groupby(metadata).count().reset_index().loc[:, ['day']] == 365).values\n",
    "\n",
    "        lookup = m.loc[:, metadata].drop_duplicates().to_numpy()\n",
    "\n",
    "        m = m.loc[:, metrics\n",
    "            ].to_numpy(\n",
    "            ).reshape(\n",
    "                -1,           # as many sites as are available\n",
    "                365,          # days\n",
    "                len(metrics)) # metrics\n",
    "\n",
    "\n",
    "        m = einops.rearrange(m, ops_string)\n",
    "\n",
    "        # let's also allow for an operations string so that we can _request_ data in a specific format instead of changing it after the fact.\n",
    "        ops = ops_string.replace(' ', '').split('->')\n",
    "        dim_order = [i\n",
    "        for e in ops[1]\n",
    "        for i in range(len(ops[0]))\n",
    "        if e == ops[0][i]\n",
    "        ]\n",
    "\n",
    "        ref = {\n",
    "            'lookup_names': metadata,\n",
    "            'data_names': metrics,\n",
    "            'data_dims': [['site', 'days', 'metrics'][i] for i in dim_order] # use the ops string to figure out the order that will be returned\n",
    "        }\n",
    "\n",
    "        return (lookup, m, ref)\n",
    "\n",
    "    def ssurgo_as_arrays(self, ops_string = 's l m -> s l m'): # site lenght (depth) metric\n",
    "        s = self.ssurgo_data.copy()\n",
    "\n",
    "        metadata = [\n",
    "            'SoilIdx',\n",
    "            ]\n",
    "        metrics = [\n",
    "            'BD',\n",
    "            'AirDry',\n",
    "            'LL15',\n",
    "            'DUL',\n",
    "            'SAT',\n",
    "            'KS',\n",
    "            'Carbon',\n",
    "            'SoilCNRatio',\n",
    "            'FOM',\n",
    "            'FOM.CN',\n",
    "            'FBiom',\n",
    "            'FInert',\n",
    "            'NO3N',\n",
    "            'NH4N',\n",
    "            'PH',\n",
    "            'ParticleSizeClay',\n",
    "            'ParticleSizeSilt',\n",
    "            'ParticleSizeSand',\n",
    "            'Maize.KL',\n",
    "            'Maize.LL',\n",
    "            'Maize.XF',\n",
    "            # 'Soybean.KL',\n",
    "            # 'Soybean.LL',\n",
    "            # 'Soybean.XF',\n",
    "            # 'Wheat.KL',\n",
    "            # 'Wheat.LL',\n",
    "            # 'Wheat.XF'\n",
    "            ]\n",
    "\n",
    "        # check that the soil slices are equally sized\n",
    "        assert len(s.Thickness.unique()) == 1\n",
    "        print(f'Soil compartment thickness: {s.Thickness[0]}')\n",
    "\n",
    "        # check that there are an equal number of slices\n",
    "        _  = s.loc[:, ['SoilIdx', 'Thickness']].groupby(['SoilIdx']).count().reset_index()\n",
    "        assert _.Thickness.min() == _.Thickness.max()\n",
    "\n",
    "        # turn range into start of slice\n",
    "        s['Depth'] = s['Depth'].str.split(pat = '-', expand = True)[0].astype(int)\n",
    "        s = s.sort_values(['SoilIdx', 'Depth']).reset_index(drop = True)\n",
    "\n",
    "        lookup = s.loc[:, metadata].drop_duplicates().to_numpy()\n",
    "\n",
    "        s = s.loc[:, metrics].to_numpy()\n",
    "        s = s.reshape(-1, lookup.shape[0], len(metrics))\n",
    "\n",
    "\n",
    "        s = einops.rearrange(s, ops_string)\n",
    "\n",
    "        # let's also allow for an operations string so that we can _request_ data in a specific format instead of changing it after the fact.\n",
    "        ops = ops_string.replace(' ', '').split('->')\n",
    "        dim_order = [i\n",
    "        for e in ops[1]\n",
    "        for i in range(len(ops[0]))\n",
    "        if e == ops[0][i]\n",
    "        ]\n",
    "\n",
    "\n",
    "        ref = {\n",
    "            'lookup_names': metadata,\n",
    "            'data_names': metrics,\n",
    "            'data_dims': [['site', 'depth', 'metrics'][i] for i in dim_order] # use the ops string to figure out the order that will be returned\n",
    "        }\n",
    "        return (lookup, s, ref)\n",
    "    \n",
    "    def genotype_as_arrays(self):\n",
    "        g = self.Genotypes.copy()\n",
    "\n",
    "        # Genotypes are easy to deal with because the table is cleaned up in filtering results. \n",
    "        metadata = [\n",
    "            'File',\n",
    "            'Genotype',\n",
    "            ]\n",
    "        metrics = [\n",
    "            'Grain.MaximumGrainsPerCob.FixedValue',\n",
    "            'Grain.MaximumPotentialGrainSize.FixedValue',\n",
    "            'Phenology.FlagLeafToFlowering.Target.FixedValue',\n",
    "            'Phenology.FloweringToGrainFilling.Target.FixedValue',\n",
    "            'Phenology.GrainFilling.Target.FixedValue',\n",
    "            'Phenology.Juvenile.Target.FixedValue',\n",
    "            'Phenology.Maturing.Target.FixedValue',\n",
    "            'Phenology.MaturityToHarvestRipe.Target.FixedValue',\n",
    "            'Phenology.Photosensitive.Target.XYPairs.X__1',\n",
    "            'Phenology.Photosensitive.Target.XYPairs.X__2',\n",
    "            'Phenology.Photosensitive.Target.XYPairs.X__3',\n",
    "            'Phenology.Photosensitive.Target.XYPairs.Y__1',\n",
    "            'Phenology.Photosensitive.Target.XYPairs.Y__2',\n",
    "            'Phenology.Photosensitive.Target.XYPairs.Y__3',\n",
    "            'Rachis.DMDemands.Structural.DMDemandFunction.MaximumOrganWt.FixedValue',\n",
    "        ]\n",
    "\n",
    "        lookup = g.loc[:, metadata]\n",
    "        lookup.File     = [self.encoder_File[e]     for e in lookup.File]\n",
    "        lookup.Genotype = [self.encoder_Genotype[e] for e in lookup.Genotype]\n",
    "        lookup = lookup.to_numpy()\n",
    "\n",
    "        g = g.loc[:, metrics].to_numpy()\n",
    "\n",
    "        ref = {\n",
    "            'lookup_names': metadata,\n",
    "            'data_names': metrics,\n",
    "            'data_dims': ('genotype', 'metrics')\n",
    "        }\n",
    "        return(lookup, g, ref)\n",
    "    \n",
    "    def ids_as_arrays(self):\n",
    "        Ids =  self.Ids.copy()\n",
    "        # apply encoder\n",
    "        Ids.Genotype = [self.encoder_Genotype[e] for e in Ids.Genotype]\n",
    "        Ids.File     = [self.encoder_File[e]     for e in Ids.File]\n",
    "        Ids.SowDate  = [self.encoder_SowDate[e]  for e in Ids.SowDate]\n",
    "\n",
    "        ref = {'lookup_names': list(Ids),}\n",
    "        Ids = Ids.to_numpy()\n",
    "        return(Ids, ref)\n",
    "\n",
    "\n",
    "x = data_helper(\n",
    "    genotypes_path = apsimx_sim_parquet_dir+'/'+'Genotypes.parquet',\n",
    "    ids_path = apsimx_sim_parquet_dir+'/'+'Ids.parquet',\n",
    "    results_path = apsimx_sim_parquet_dir,\n",
    "    ssurgo_path = '/home/Shared/apsimx_env_data/apsimx_i_soil_ssurgo_profile.parquet',\n",
    "    met_path    = '/home/Shared/apsimx_env_data/apsimx_i_soil_POWER_met.parquet'\n",
    "    )\n",
    "\n",
    "# restrict to simulated cultivars with the maximum MaximumGrainsPerCob\n",
    "mask = (x.Genotypes['Grain.MaximumGrainsPerCob.FixedValue'] == x.Genotypes['Grain.MaximumGrainsPerCob.FixedValue'].max())\n",
    "\n",
    "print(f'Before Mask: Genotypes: {x.Genotypes.shape} Ids: {x.Ids.shape}')\n",
    "x.apply_mask(table='Genotypes', mask=mask)\n",
    "print(f'After Mask: Genotypes: {x.Genotypes.shape} Ids: {x.Ids.shape}')\n",
    "\n",
    "x.setup_encoders()\n",
    "x.load_results(years = [1990, 2000, 2010, 2020], dry_run = False)\n",
    "x.filter_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soil compartment thickness: 200.0\n"
     ]
    }
   ],
   "source": [
    "result_lookup,   result,   result_ref   =  x.results_as_arrays(output_type = 'point')\n",
    "\n",
    "genotype_lookup, genotype, genotype_ref =  x.genotype_as_arrays()\n",
    "met_lookup,      met,      met_ref      =  x.met_as_arrays(   ops_string = 's d m -> s d m')\n",
    "ssurgo_lookup,   ssurgo,   ssurgo_ref   =  x.ssurgo_as_arrays(ops_string = 's l m -> s l m')\n",
    "ids_lookup,                ids_ref      = x.ids_as_arrays()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2],\n",
       "        [3, 3],\n",
       "        [4, 4]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_113814/3161855360.py:67: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  met             = torch.tensor(met),\n",
      "/tmp/ipykernel_113814/3161855360.py:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  ssurgo          = torch.tensor(ssurgo),\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[152], line 73\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (y, cultivar, ssurgo, met)\n\u001b[1;32m     60\u001b[0m dat \u001b[38;5;241m=\u001b[39m apsimxDataset(\n\u001b[1;32m     61\u001b[0m     result_lookup   \u001b[38;5;241m=\u001b[39m result_lookup,   \n\u001b[1;32m     62\u001b[0m     result          \u001b[38;5;241m=\u001b[39m result,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     69\u001b[0m     ssurgo          \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(ssurgo),\n\u001b[1;32m     70\u001b[0m     )\n\u001b[0;32m---> 73\u001b[0m y, cultivar, ssurgo, met \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(dat))\n",
      "\u001b[0;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class apsimxDataset(Dataset):\n",
    "    def __init__(\n",
    "            self, \n",
    "            result_lookup,   result,\n",
    "            ids_lookup,\n",
    "            genotype_lookup, genotype,\n",
    "            met_lookup,      met,\n",
    "            ssurgo_lookup,   ssurgo,\n",
    "            ):\n",
    "        # super().__init__()\n",
    "        self.result_lookup = result_lookup\n",
    "        self.result = result\n",
    "        self.ids_lookup = ids_lookup\n",
    "        self.genotype_lookup = genotype_lookup\n",
    "        self.genotype = genotype\n",
    "        self.met_lookup = met_lookup\n",
    "        self.met = met\n",
    "        self.ssurgo_lookup = ssurgo_lookup\n",
    "        self.ssurgo = ssurgo\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.result_lookup.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # setup keys\n",
    "        # get keys from y\n",
    "        FactorialUID, Year = self.result_lookup[idx, ]\n",
    "        # get use ids to get env keys from Ids table\n",
    "        Longitude, Latitude, SoilIdx, File, Genotype, SowDate, FactorialUID = self.ids_lookup[(self.ids_lookup[:, 6] == int(FactorialUID)), ].squeeze()\n",
    "        # now we have the full set of keys.\n",
    "\n",
    "        #TODO use SowDate to spike in planting\n",
    "\n",
    "        # get values\n",
    "        y = self.result[idx]\n",
    "\n",
    "        # cultivar variables\n",
    "        mask = (\n",
    "            (self.genotype_lookup[:, 0] == File) & \n",
    "            (self.genotype_lookup[:, 1] == Genotype)\n",
    "            )\n",
    "        cultivar = self.genotype[mask, ]\n",
    "\n",
    "        # ssurgo data\n",
    "        mask = (self.ssurgo_lookup[:, 0] == int(SoilIdx))\n",
    "        ssurgo = self.ssurgo[:, mask, :]\n",
    "\n",
    "        # met data\n",
    "        mask = (\n",
    "            (self.met_lookup[:, 0] == Longitude) & \n",
    "            (self.met_lookup[:, 1] == Latitude) & \n",
    "            (self.met_lookup[:, 2] == float(Year)) \n",
    "            )\n",
    "        met = self.met[mask, :, :]\n",
    "\n",
    "        return (y, cultivar, ssurgo, met)\n",
    "\n",
    "dat = apsimxDataset(\n",
    "    result_lookup   = result_lookup,   \n",
    "    result          = result,\n",
    "    ids_lookup      = torch.tensor(ids_lookup),\n",
    "    genotype_lookup = torch.tensor(genotype_lookup), \n",
    "    genotype        = torch.tensor(genotype),\n",
    "    met_lookup      = torch.tensor(met_lookup),      \n",
    "    met             = torch.tensor(met),\n",
    "    ssurgo_lookup   = torch.tensor(ssurgo_lookup),   \n",
    "    ssurgo          = torch.tensor(ssurgo),\n",
    "    )\n",
    "\n",
    "\n",
    "y, cultivar, ssurgo, met = next(iter(dat))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[  1.73      ,  -2.63      ,  -9.07      , ...,   4.66      ,\n",
       "           8.66111337,  10.56250193],\n",
       "        [  6.13      ,  -0.34      , -10.4       , ...,   5.23      ,\n",
       "           8.66111337,  10.56250193],\n",
       "        [  6.2       ,   1.73      ,  -4.42      , ...,   5.41      ,\n",
       "           8.66111337,  10.56250193],\n",
       "        ...,\n",
       "        [  2.16      ,   4.19      ,  -1.3       , ...,   4.27      ,\n",
       "           8.66111337,  10.56250193],\n",
       "        [  3.08      ,  -0.79      , -12.48      , ...,   3.06      ,\n",
       "           8.66111337,  10.56250193],\n",
       "        [  7.16      ,  -7.64      , -14.86      , ...,   3.71      ,\n",
       "           8.66111337,  10.56250193]]])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# core of the dataloader\n",
    "result_lookup.shape[0] # number of ys\n",
    "\n",
    "i = 0\n",
    "\n",
    "# y variable\n",
    "result[i]\n",
    "\n",
    "# x variables\n",
    "## \n",
    "# get keys from y\n",
    "FactorialUID, Year = result_lookup[i, ]\n",
    "\n",
    "# get use ids to get env keys from Ids table\n",
    "Longitude, Latitude, SoilIdx, File, Genotype, SowDate, FactorialUID = Ids[(Ids[:, 6] == int(FactorialUID)), ].squeeze()\n",
    "\n",
    "# now we have the full set of keys.\n",
    "\n",
    "# cultivar variables\n",
    "mask = (\n",
    "    (genotype_lookup[:, 0] == File) & \n",
    "    (genotype_lookup[:, 1] == Genotype)\n",
    "    )\n",
    "\n",
    "genotype[mask, ]\n",
    "\n",
    "# ssurgo data\n",
    "mask = (ssurgo_lookup[:, 0] == int(SoilIdx))\n",
    "ssurgo[:, mask, :]\n",
    "\n",
    "# met data\n",
    "\n",
    "mask = (\n",
    "    (met_lookup[:, 0] == Longitude) & \n",
    "    (met_lookup[:, 1] == Latitude) & \n",
    "    (met_lookup[:, 2] == float(Year)) \n",
    "    )\n",
    "\n",
    "met[mask, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate data_helper so that we can look at how the methods in data_helper work\n",
    "x = data_helper(\n",
    "    genotypes_path = apsimx_sim_parquet_dir+'/'+'Genotypes.parquet',\n",
    "    ids_path = apsimx_sim_parquet_dir+'/'+'Ids.parquet',\n",
    "    results_path = apsimx_sim_parquet_dir,\n",
    "    ssurgo_path = '/home/Shared/apsimx_env_data/apsimx_i_soil_ssurgo_profile.parquet',\n",
    "    met_path    = '/home/Shared/apsimx_env_data/apsimx_i_soil_POWER_met.parquet'\n",
    "    )\n",
    "\n",
    "# restrict to simulated cultivars with the maximum MaximumGrainsPerCob\n",
    "mask = (x.Genotypes['Grain.MaximumGrainsPerCob.FixedValue'] == x.Genotypes['Grain.MaximumGrainsPerCob.FixedValue'].max())\n",
    "x.apply_mask(table='Genotypes', mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = x.met_data\n",
    "metadata = [\n",
    "    'Longitude', \n",
    "    'Latitude', \n",
    "    'year'\n",
    "    ]\n",
    "metrics = [\n",
    "    'radn_MJ_div_m2_div_day',\n",
    "    'maxt_oC',\n",
    "    'mint_oC',\n",
    "    'rain_mm',\n",
    "    'rh_pr',\n",
    "    'windspeed_m_div_s',\n",
    "    'tav',\n",
    "    'amp'\n",
    "    ]\n",
    "\n",
    "# make sure that there are the expected number of values per group\n",
    "m = m.loc[(m.day < 366), ].sort_values(metadata).reset_index(drop = True)\n",
    "m = m.groupby(metadata+['day']).mean().reset_index()\n",
    "\n",
    "# all groups have 365 obs?\n",
    "assert False not in (m.groupby(metadata).count().reset_index().loc[:, ['day']] == 365).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate reshaping. We want to \"fold\" this table so that there is a site (lon/lat/year) axis, a day axis, and a metrics axis.\n",
    "\n",
    "# To demonstrate this we'll get the \n",
    "mn = m.loc[:, metadata+['day']].to_numpy()\n",
    "\n",
    "mn = mn.reshape(-1,  # as many sites as are available\n",
    "                365, # days\n",
    "                4)   # metrics (in this case lon, lat, year, day)\n",
    "\n",
    "import plotly.express as px\n",
    "print('Here we\\'ll show that this reshape is working as expected by plotting the \"day\" column across several sites')\n",
    "px.imshow(\n",
    "    np.concatenate([\n",
    "        mn[i, 0:10, 3:]\n",
    "        for i in range(10)\n",
    "    ], axis = 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = x.met_data\n",
    "metadata = [\n",
    "    'Longitude', \n",
    "    'Latitude', \n",
    "    'year'\n",
    "    ]\n",
    "metrics = [\n",
    "    'radn_MJ_div_m2_div_day',\n",
    "    'maxt_oC',\n",
    "    'mint_oC',\n",
    "    'rain_mm',\n",
    "    'rh_pr',\n",
    "    'windspeed_m_div_s',\n",
    "    'tav',\n",
    "    'amp'\n",
    "    ]\n",
    "\n",
    "\n",
    "# make sure that there are the expected number of values per group\n",
    "m = m.loc[(m.day < 366), ].sort_values(metadata).reset_index(drop = True)\n",
    "m = m.groupby(metadata+['day']).mean().reset_index()\n",
    "\n",
    "# all groups have 365 obs?\n",
    "assert False not in (m.groupby(metadata).count().reset_index().loc[:, ['day']] == 365).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "m = m.loc[:, metrics\n",
    "    ].to_numpy(\n",
    "    ).reshape(\n",
    "        -1,           # as many sites as are available\n",
    "        365,          # days\n",
    "        len(metrics)) # metrics\n",
    "\n",
    "m[0, 0:5, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's also allow for an operations string so that we can _request_ data in a specific format instead of changing it after the fact.\n",
    "ops_string = 's d m -> s m d'\n",
    "\n",
    "ops = ops_string.replace(' ', '').split('->')\n",
    "\n",
    "\n",
    "dim_order = [i\n",
    " for e in ops[1]\n",
    " for i in range(len(ops[0]))\n",
    " if e == ops[0][i]\n",
    " ]\n",
    "\n",
    "\n",
    "m.shape, einops.rearrange(m, ops_string).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = {\n",
    "    'lookup_names': metadata,\n",
    "    'data_names': metrics,\n",
    "    'data_dims': [['site', 'days', 'metrics'][i] for i in dim_order] # use the ops string to figure out the order that will be returned\n",
    "}\n",
    "# to help avoid confustion later, we'll return some reference information too\n",
    "# (lookup, m, ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.ssurgo_data = self.ssurgo_data.reset_index(drop = True)\n",
    "s = x.ssurgo_data\n",
    "\n",
    "metadata = [\n",
    "    'SoilIdx',\n",
    "    ]\n",
    "metrics = [\n",
    "    'BD',\n",
    "    'AirDry',\n",
    "    'LL15',\n",
    "    'DUL',\n",
    "    'SAT',\n",
    "    'KS',\n",
    "    'Carbon',\n",
    "    'SoilCNRatio',\n",
    "    'FOM',\n",
    "    'FOM.CN',\n",
    "    'FBiom',\n",
    "    'FInert',\n",
    "    'NO3N',\n",
    "    'NH4N',\n",
    "    'PH',\n",
    "    'ParticleSizeClay',\n",
    "    'ParticleSizeSilt',\n",
    "    'ParticleSizeSand',\n",
    "    'Maize.KL',\n",
    "    'Maize.LL',\n",
    "    'Maize.XF',\n",
    "    # 'Soybean.KL',\n",
    "    # 'Soybean.LL',\n",
    "    # 'Soybean.XF',\n",
    "    # 'Wheat.KL',\n",
    "    # 'Wheat.LL',\n",
    "    # 'Wheat.XF'\n",
    "    ]\n",
    "\n",
    "s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the soil slices are equally sized\n",
    "assert len(s.Thickness.unique()) == 1\n",
    "print(f'Soil compartment thickness: {s.Thickness[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that there are an equal number of slices\n",
    "_  = s.loc[:, ['SoilIdx', 'Thickness']].groupby(['SoilIdx']).count().reset_index()\n",
    "assert _.Thickness.min() == _.Thickness.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn range into start of slice\n",
    "s['Depth'] = s['Depth'].str.split(pat = '-', expand = True)[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.sort_values(['SoilIdx', 'Depth']).reset_index(drop = True)\n",
    "\n",
    "lookup = s.loc[:, metadata].drop_duplicates().to_numpy()\n",
    "\n",
    "s = s.loc[:, metrics].to_numpy()\n",
    "s = s.reshape(-1, lookup.shape[0], len(metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = {\n",
    "    'lookup_names': metadata,\n",
    "    'data_names': metrics,\n",
    "    'data_dims': ('site', 'depth', 'metrics')\n",
    "}\n",
    "# to help avoid confustion later, we'll return some reference information too\n",
    "# (lookup, s, ref)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self.Genotypes   = self.Genotypes.reset_index(  drop = True)\n",
    "g = x.Genotypes\n",
    "\n",
    "# Genotypes are easy to deal with because the table is cleaned up in filtering results. \n",
    "\n",
    "g.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metadata = [\n",
    "    'File',\n",
    "    'Genotype',\n",
    "    ]\n",
    "metrics = [\n",
    "    'Grain.MaximumGrainsPerCob.FixedValue',\n",
    "    'Grain.MaximumPotentialGrainSize.FixedValue',\n",
    "    'Phenology.FlagLeafToFlowering.Target.FixedValue',\n",
    "    'Phenology.FloweringToGrainFilling.Target.FixedValue',\n",
    "    'Phenology.GrainFilling.Target.FixedValue',\n",
    "    'Phenology.Juvenile.Target.FixedValue',\n",
    "    'Phenology.Maturing.Target.FixedValue',\n",
    "    'Phenology.MaturityToHarvestRipe.Target.FixedValue',\n",
    "    'Phenology.Photosensitive.Target.XYPairs.X__1',\n",
    "    'Phenology.Photosensitive.Target.XYPairs.X__2',\n",
    "    'Phenology.Photosensitive.Target.XYPairs.X__3',\n",
    "    'Phenology.Photosensitive.Target.XYPairs.Y__1',\n",
    "    'Phenology.Photosensitive.Target.XYPairs.Y__2',\n",
    "    'Phenology.Photosensitive.Target.XYPairs.Y__3',\n",
    "    'Rachis.DMDemands.Structural.DMDemandFunction.MaximumOrganWt.FixedValue',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = {\n",
    "    'lookup_names': metadata,\n",
    "    'data_names': metrics,\n",
    "    'data_dims': ('genotype', 'metrics')\n",
    "}\n",
    "# (g.loc[:, metadata].to_numpy(), g.loc[:, metrics].to_numpy(), ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Example of Internals:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Masking genotypes\n",
    "\n",
    "The goal here is to not reinvent the wheel but to make certain tasks easier. We're storing several tables as pandas dataframes we can lean on pandas to filter the data. Filters should be applied to multiple tables so we'll automate that part to avoid forgetting to do it. The work flow will be \n",
    "1. Access a dataframe and generate a mask\n",
    "2. Use a helper function to apply the mask\n",
    "    1. The helper function will filter the specified dataframe (Ids or Genotypes)\n",
    "    1. The helper will then use the filtered dataframe to filter the other one keeping them both in sync. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's select a region of the country. For this demonstration I'll filter to a region around Columbia MO.\n",
    "\n",
    "mask = ((x.Ids.Longitude < -90) & \n",
    "        (x.Ids.Longitude > -95) &\n",
    "        (x.Ids.Latitude  <  40) & \n",
    "        (x.Ids.Latitude  >  30) \n",
    "        )\n",
    "\n",
    "print(f'Before Mask: Genotypes: {x.Genotypes.shape} Ids: {x.Ids.shape}')\n",
    "x.apply_mask(table='Ids', mask=mask)\n",
    "print(f'After Mask: Genotypes: {x.Genotypes.shape} Ids: {x.Ids.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.Genotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Results tables\n",
    "\n",
    "The simulation results are spread across ~134 parquet files. This reduces the data storage needs but makes it harder to work with the files (than having one file per simulation or a fully fledged database server). To make the process of loading data faster, we find all the unique parquet files to be read (we might want multiple simulations from one file) and then read them in turn. Furthermore, we might only want to use a subset of years so we'll allow for this to be specified.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.load_results(years = [1990, 2000, 2010, 2020], dry_run = True) # years is optional. If an empty list is passed all years will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.load_results(years = [1990, 2000, 2010, 2020], dry_run = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now results can be accessed \n",
    "x.results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove run-on simulation results and return each year's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn results into tensor\n",
    "# Convert unix date to year/doy\n",
    "tmp = x.results.merge(_prep_unix_epoch_to_date(max_year = 2025).loc[:, ['Unix', 'Year', 'DOY']].rename(columns={'Unix':'Date'}))\n",
    "\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Masking genotypes\n",
    "\n",
    "The goal here is to not reinvent the wheel but to make certain tasks easier. We're storing several tables as pandas dataframes we can lean on pandas to filter the data. Filters should be applied to multiple tables so we'll automate that part to avoid forgetting to do it. The work flow will be \n",
    "1. Access a dataframe and generate a mask\n",
    "2. Use a helper function to apply the mask\n",
    "    1. The helper function will filter the specified dataframe (Ids or Genotypes)\n",
    "    1. The helper will then use the filtered dataframe to filter the other one keeping them both in sync. \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's select a region of the country. For this demonstration I'll filter to a region around Columbia MO.\n",
    "\n",
    "mask = ((x.Ids.Longitude < -90) & \n",
    "        (x.Ids.Longitude > -95) &\n",
    "        (x.Ids.Latitude  <  40) & \n",
    "        (x.Ids.Latitude  >  30) \n",
    "        )\n",
    "\n",
    "print(f'Before Mask: Genotypes: {x.Genotypes.shape} Ids: {x.Ids.shape}')\n",
    "x.apply_mask(table='Ids', mask=mask)\n",
    "print(f'After Mask: Genotypes: {x.Genotypes.shape} Ids: {x.Ids.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.Genotypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Results tables\n",
    "\n",
    "The simulation results are spread across ~134 parquet files. This reduces the data storage needs but makes it harder to work with the files (than having one file per simulation or a fully fledged database server). To make the process of loading data faster, we find all the unique parquet files to be read (we might want multiple simulations from one file) and then read them in turn. Furthermore, we might only want to use a subset of years so we'll allow for this to be specified.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.load_results(years = [1990, 2000, 2010, 2020], dry_run = True) # years is optional. If an empty list is passed all years will be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.load_results(years = [1990, 2000, 2010, 2020], dry_run = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now results can be accessed \n",
    "x.results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove run-on simulation results and return each year's results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn results into tensor\n",
    "# Convert unix date to year/doy\n",
    "tmp = x.results.merge(_prep_unix_epoch_to_date(max_year = 2025).loc[:, ['Unix', 'Year', 'DOY']].rename(columns={'Unix':'Date'}))\n",
    "\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure we don't allow for observations to cross over two years\n",
    "# tmp2 = tmp.loc[:, ['FactorialUID', 'Year']].drop_duplicates()\n",
    "\n",
    "tmp.loc[tmp.FactorialUID == 14689, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "_ = tmp.loc[tmp.FactorialUID == 14689, ]\n",
    "plt.scatter(x = _.DOY, y = _['Maize.AboveGround.Wt'])\n",
    "\n",
    "# some of these run into the subsequent year. To deal with this..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the smallest above ground weight within each year, usethat to get the first date within eacy year and then filter for the values that are >= the within year start.\n",
    "_ = tmp.loc[:, ['FactorialUID', 'Year', 'Maize.AboveGround.Wt']].groupby(['FactorialUID', 'Year']).agg('min').reset_index()\n",
    "_ = _.merge(tmp).loc[:, ['FactorialUID', 'Year', 'Date']].rename(columns={'Date': 'YearStart'})\n",
    "tmp = tmp.merge(_, how='left')\n",
    "tmp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retained values\n",
    "_ = tmp.loc[((tmp.FactorialUID == 14689) & \n",
    "             (tmp.Date >= tmp.YearStart)\n",
    "             ), ]\n",
    "plt.scatter(x = _.DOY, y = _['Maize.AboveGround.Wt'])\n",
    "\n",
    "_ = tmp.loc[((tmp.FactorialUID == 14689) & \n",
    "             (tmp.Date < tmp.YearStart)\n",
    "             ), ]\n",
    "plt.scatter(x = _.DOY, y = _['Maize.AboveGround.Wt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn results into tensor\n",
    "tmp = x.results.merge(_prep_unix_epoch_to_date(max_year = 2025).loc[:, ['Unix', 'Year', 'DOY']].rename(columns={'Unix':'Date'}))\n",
    "\n",
    "# find the smallest above ground weight within each year, usethat to get the first date within eacy year and then filter for the values that are >= the within year start.\n",
    "_ = tmp.loc[:, ['FactorialUID', 'Year', 'Maize.AboveGround.Wt']].groupby(['FactorialUID', 'Year']).agg('min').reset_index()\n",
    "_ = _.merge(tmp).loc[:, ['FactorialUID', 'Year', 'Date']].rename(columns={'Date': 'YearStart'})\n",
    "tmp = tmp.merge(_, how='left')\n",
    "\n",
    "tmp = tmp.loc[(tmp.Date >= tmp.YearStart), ]\n",
    "\n",
    "tmp=tmp.drop(columns=['YearStart', 'Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if making point estimates:\n",
    "tmp.drop(columns='DOY').groupby(['FactorialUID', 'Year']).agg('max').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (tmp.FactorialUID == 11811) & \n",
    "    (tmp.Year == 1990))\n",
    "\n",
    "min_doy, max_doy = tmp.loc[mask, 'DOY'].agg(('min', 'max')).values\n",
    "\n",
    "out = torch.zeros(365, 3)\n",
    "out[min_doy:(max_doy+1), ] = torch.tensor(tmp.loc[mask, ['Maize.AboveGround.Wt', 'Maize.LAI', 'yield_Kgha']].to_numpy())\n",
    "\n",
    "# propagate forward observations to all dates following the max\n",
    "out[max_doy:365, ] = out[max_doy, ]\n",
    "\n",
    "plt.scatter(x = np.linspace(0, 365, 365), y = out[:, 2].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.results_as_arrays(output_type = 'point')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.results_as_arrays(output_type = 'sequence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO write a method to return the \n",
    "# - cultivar information \n",
    "# - planting date as DOY\n",
    "# - lookup(s)\n",
    "\n",
    "#TODO work with the environmental data\n",
    "# - reduce columns\n",
    "# - create easy lookup \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up Environmental Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_path = '/home/Shared/apsimx_env_data'+'/'\n",
    "\n",
    "# ./apsimx_i_soil_ssurgo_profile.parquet\n",
    "\n",
    "# met files (easy to parse)\n",
    "# env_path+'apsimx_i_soil_POWER_met.parquet'\n",
    "\n",
    "lookup = pq.read_table(env_path+'apsimx_i_soil_gps.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load env data\n",
    "met_data = pq.read_table(env_path+'apsimx_i_soil_POWER_met.parquet').to_pandas().rename(columns = {'latitude':'Latitude', \n",
    "                                                                                                   'longitude': 'Longitude'})\n",
    "met_data\n",
    "\n",
    "ssurgo_data = pq.read_table(env_path+'apsimx_i_soil_ssurgo_profile.parquet').to_pandas().rename(columns = {'soil_i':'SoilIdx'})\n",
    "ssurgo_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shared soilIdx\n",
    "\n",
    "Ids_idxs = x.Ids.loc[:, ['Longitude', 'Latitude', 'SoilIdx']].drop_duplicates()\n",
    "ssurgo_s = ssurgo_data.loc[:, ['SoilIdx']].drop_duplicates()\n",
    "met_lonlat = met_data.loc[:, ['Longitude', 'Latitude']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Uniq. Lon/Lat/Soil: {Ids_idxs.shape[0]}')\n",
    "print('Condition on data availability:')\n",
    "Ids_idxs = Ids_idxs.merge(ssurgo_s, how = 'inner').merge(met_lonlat, how = 'inner')\n",
    "print(f'Uniq. Lon/Lat/Soil: {Ids_idxs.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce the environmental datasets:\n",
    "met_data    = Ids_idxs.drop(columns=['SoilIdx']).drop_duplicates().merge(met_lonlat, how = 'inner')\n",
    "ssurgo_data = Ids_idxs.drop(columns=['Longitude', 'Latitude']).drop_duplicates().merge(ssurgo_data, how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update Ids & results:\n",
    "x.Ids = Ids_idxs.merge(x.Ids)\n",
    "\n",
    "_ = x.Ids.loc[:, ['FactorialUID']].drop_duplicates()\n",
    "x.results = _.merge(x.results, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids_s    = x.Ids.loc[:, ['SoilIdx']].drop_duplicates()\n",
    "x.Ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using inner join here to make sure there aren't any indexes in the results that we don't have environmental data for\n",
    "ssurgo_data.merge(Ids_s, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids_s.merge(ssurgo_data, how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physical Properties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_tables = [\n",
    "    'apsimx_i_soil_Soils_CERESSoilTemperature.parquet',\n",
    "    'apsimx_i_soil_Soils_Chemical.parquet',\n",
    "    'apsimx_i_soil_Soils_Nutrients.Nutrient.parquet',\n",
    "    'apsimx_i_soil_Soils_Organic.parquet',\n",
    "    'apsimx_i_soil_Soils_Physical.parquet',\n",
    "    'apsimx_i_soil_Soils_Soil.parquet',\n",
    "    'apsimx_i_soil_Soils_Solute.parquet',\n",
    "    'apsimx_i_soil_Soils_Water.parquet',\n",
    "    'apsimx_i_soil_WaterModel_WaterBalance.parquet']\n",
    "\n",
    "soil_dict = {e:pq.read_table(env_path+e).to_pandas() for e in env_tables}#[env_tables[1]]\n",
    "\n",
    "# focus on only one entry\n",
    "soil_dict = {k:soil_dict[k].loc[(soil_dict[k].list_idx == 1), ] for k in env_tables}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(set(sum([list(pq.read_table(env_path+e).to_pandas()) for e in env_tables], []))))\n",
    "\n",
    "# problem: There are some columns that just don't look like they are saved in the data. E.g. soil/Physical.`maize LL` \n",
    "\n",
    "['$type',\n",
    " 'AirDry',\n",
    " 'AirDryMetadata',\n",
    " 'ApsoilNumber',\n",
    " 'BD',\n",
    " 'BDMetadata',\n",
    " 'CN2Bare',\n",
    " 'CNCov',\n",
    " 'CNRed',\n",
    " 'Carbon',\n",
    " 'CarbonUnits',\n",
    " 'CatchmentArea',\n",
    " 'Comments',\n",
    " 'Country',\n",
    " 'D0',\n",
    " 'DUL',\n",
    " 'DULMetadata',\n",
    " 'DataSource',\n",
    " 'DepthConstant',\n",
    " 'DiffusConst',\n",
    " 'DiffusSlope',\n",
    " 'DischargeWidth',\n",
    " 'Enabled',\n",
    " 'Exco',\n",
    " 'FBiom',\n",
    " 'FIP',\n",
    " 'FInert',\n",
    " 'FOM',\n",
    " 'FOMCNRatio',\n",
    " 'FilledFromTop',\n",
    " 'InitialPAWmm',\n",
    " 'InitialValues',\n",
    " 'InitialValuesUnits',\n",
    " 'KS',\n",
    " 'LL15',\n",
    " 'LL15Metadata',\n",
    " 'Latitude',\n",
    " 'Longitude',\n",
    " 'MaxDepthSoluteAccessible',\n",
    " 'MaxEffectiveRunoff',\n",
    " 'Name',\n",
    " 'PH',\n",
    " 'PHMetadata',\n",
    " 'PHUnits',\n",
    " 'PSIDul',\n",
    " 'ParticleSizeClay',\n",
    " 'ParticleSizeSand',\n",
    " 'ParticleSizeSilt',\n",
    " 'ReadOnly',\n",
    " 'RecordNumber',\n",
    " 'Region',\n",
    " 'RelativeTo',\n",
    " 'ResourceName',\n",
    " 'RunoffEffectivenessAtMovingSolute',\n",
    " 'SAT',\n",
    " 'SATMetadata',\n",
    " 'SWCON',\n",
    " 'Salb',\n",
    " 'SoilCNRatio',\n",
    " 'SoilType',\n",
    " 'State',\n",
    " 'SummerCona',\n",
    " 'SummerDate',\n",
    " 'SummerU',\n",
    " 'Thickness',\n",
    " 'WaterTableConcentration',\n",
    " 'WinterCona',\n",
    " 'WinterDate',\n",
    " 'WinterU',\n",
    " 'YearOfSampling',\n",
    " 'list_idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_dict['apsimx_i_soil_Soils_Physical.parquet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table to cols:\n",
    "desired_fields = {\n",
    "# 'apsimx_i_soil_Soils_CERESSoilTemperature.parquet': [],\n",
    "'apsimx_i_soil_Soils_Chemical.parquet': ['Thickness', 'PH'],\n",
    "# 'apsimx_i_soil_Soils_Nutrients.Nutrient.parquet': [],\n",
    "'apsimx_i_soil_Soils_Organic.parquet': ['Thickness', 'Carbon', 'SoilCNRatio', 'FBiom', 'FInert', 'FOM', 'FOMCNRatio', 'CarbonUnits'],\n",
    "'apsimx_i_soil_Soils_Physical.parquet': ['Thickness', 'ParticleSizeClay', 'ParticleSizeSand', 'ParticleSizeSilt', 'BD', 'AirDry', 'LL15', 'DUL', 'SAT'],\n",
    "# 'apsimx_i_soil_Soils_Soil.parquet': [],\n",
    "# 'apsimx_i_soil_Soils_Solute.parquet': [],\n",
    "'apsimx_i_soil_Soils_Water.parquet': ['Thickness', 'InitialValues', 'InitialPAWmm', 'RelativeTo'],\n",
    "'apsimx_i_soil_WaterModel_WaterBalance.parquet': ['Thickness', 'SWCON']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = [soil_dict[k].loc[:, desired_fields[k]] for i, k in enumerate(desired_fields)]\n",
    "\n",
    "\n",
    "# pd.merge( res[1])\n",
    "res[0]\n",
    "res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soil_dict[env_tables[1]] \t\n",
    "soil_dict[env_tables[3]] \t\n",
    "\n",
    "# soil_dict['apsimx_i_soil_Soils_CERESSoilTemperature.parquet']\n",
    "soil_dict['apsimx_i_soil_Soils_Chemical.parquet'] # Thickness\tPH \n",
    "# soil_dict['apsimx_i_soil_Soils_Nutrients.Nutrient.parquet']\n",
    "soil_dict['apsimx_i_soil_Soils_Organic.parquet'] #Thickness\tCarbon\tSoilCNRatio\tFBiom\tFInert\tFOM\t$type\tFOMCNRatio\tCarbonUnits\n",
    "soil_dict['apsimx_i_soil_Soils_Physical.parquet'] # \tThickness\tParticleSizeClay\tParticleSizeSand\tParticleSizeSilt\tBD\tAirDry\tLL15\tDUL\tSAT\t\n",
    "# soil_dict['apsimx_i_soil_Soils_Soil.parquet'] # contains metadata\n",
    "# soil_dict['apsimx_i_soil_Soils_Solute.parquet'] # # Maybe useful, but it dosn't look like it? Thickness\tInitialValues\tExco\tFIP\t\tInitialValuesUnits\tWaterTableConcentration\tD0\tDepthConstant\tMaxDepthSoluteAccessible\tRunoffEffectivenessAtMovingSolute\tMaxEffectiveRunoff\tName\n",
    "soil_dict['apsimx_i_soil_Soils_Water.parquet'] # Thickness\tInitialValues\t$type\tInitialPAWmm\tRelativeTo\n",
    "soil_dict['apsimx_i_soil_WaterModel_WaterBalance.parquet'] # \tThickness\tSWCON\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depth\tThickness\tBD\tAirDry\tLL15\tDUL\tSAT\tKS\tCarbon\tSoilCNRatio\tFOM\tFOM.CN\tFBiom\tFInert\tNO3N\tNH4N\tPH\tParticleSizeClay\tParticleSizeSilt\tParticleSizeSand\tMaize.KL\tMaize.LL\tMaize.XF\tSoybean.KL\tSoybean.LL\tSoybean.XF\tWheat.KL\tWheat.LL\tWheat.XF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define working set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with a location and year\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter_mapbox(Ids.loc[:, ['Longitude', 'Latitude']].drop_duplicates(), lon = 'Longitude', lat = 'Latitude',\n",
    "                        color_discrete_sequence=[\"fuchsia\"], zoom=3, height=300)\n",
    "fig.update_layout(mapbox_style=\"open-street-map\")\n",
    "fig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -> File\tGenotype  --Genotypes-> \n",
    "#\t FactorialUID     --Results->\n",
    "\n",
    "lon, lat, soil = [-76.65611874999999, 42.733264, 141] #lon lat soil\n",
    "\n",
    "# should allow ranges, slices, or all\n",
    "sow = '19-Jun'\n",
    "# allow all\n",
    "cultivar = 'Cultivar1'\n",
    "\n",
    "\n",
    "# -> File FactorialUID\n",
    "\n",
    "mask = (\n",
    "    (Ids.Longitude == lon) &\n",
    "    (Ids.Latitude == lat) &\n",
    "    (Ids.SoilIdx == soil) &\n",
    "    (Ids.SowDate == sow) &\n",
    "    (Ids.Genotype == cultivar))\n",
    "\n",
    "Ids.loc[mask, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting with a target set of genotypes\n",
    "mask = Genotypes['Grain.MaximumGrainsPerCob.FixedValue'] == Genotypes['Grain.MaximumGrainsPerCob.FixedValue'].max()\n",
    "\n",
    "Genotypes.loc[mask, ['File', 'Genotype']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Small Datasets to Tensors\n",
    "### `Genotypes` (Cultivar variables)\n",
    "\n",
    "Warning! There are some NAs from Genotypes that are not \"Cultivar\\d+\" Genotypes. These are calibrated genotypes that with defaults that are not clear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep as df or matrix (contains text)\n",
    "Genotypes_lookup = Genotypes.loc[:, ['File', 'Genotype']].copy()\n",
    "\n",
    "# ['Grain.MaximumGrainsPerCob.FixedValue',\n",
    "#  'Grain.MaximumPotentialGrainSize.FixedValue',\n",
    "#  'Phenology.FlagLeafToFlowering.Target.FixedValue',\n",
    "#  'Phenology.FloweringToGrainFilling.Target.FixedValue',\n",
    "#  'Phenology.GrainFilling.Target.FixedValue',\n",
    "#  'Phenology.Juvenile.Target.FixedValue',\n",
    "#  'Phenology.Maturing.Target.FixedValue',\n",
    "#  'Phenology.MaturityToHarvestRipe.Target.FixedValue',\n",
    "#  'Phenology.Photosensitive.Target.XYPairs.X__1',\n",
    "#  'Phenology.Photosensitive.Target.XYPairs.X__2',\n",
    "#  'Phenology.Photosensitive.Target.XYPairs.X__3',\n",
    "#  'Phenology.Photosensitive.Target.XYPairs.Y__1',\n",
    "#  'Phenology.Photosensitive.Target.XYPairs.Y__2',\n",
    "#  'Phenology.Photosensitive.Target.XYPairs.Y__3',\n",
    "#  'Rachis.DMDemands.Structural.DMDemandFunction.MaximumOrganWt.FixedValue']\n",
    "\n",
    "Genotypes_cols = list(Genotypes)\n",
    "Genotypes = Genotypes.drop(columns=['File', 'Genotype'])\n",
    "# coerce None to NaN so we can convert to matrix\n",
    "for e in list(Genotypes):\n",
    "    Genotypes[e] = Genotypes[e].astype(float)\n",
    "\n",
    "# Genotypes = torch.tensor(Genotypes.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Genotypes_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for a given idx and year...\n",
    "idx_Ids = 1\n",
    "year = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get lookup information\n",
    "lookup = Ids.loc[idx_Ids, ].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = ((Genotypes_lookup.File == lookup['File']) &  (Genotypes_lookup.Genotype == lookup['Genotype']))\n",
    "# should only have a single value\n",
    "assert sum(mask) == 1\n",
    "\n",
    "idx_Geno = Genotypes_lookup.loc[mask, ].index[0]\n",
    "\n",
    "Genotypes[idx_Geno]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup_date = _prep_unix_epoch_to_date(max_year = 2024)\n",
    "lookup_date.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result = pq.read_table(apsimx_sim_parquet_dir+'/'+'sim_1698440407_4739.parquet').to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup_* is a internally generated ref\n",
    "# *_lookup is a table based on loaded apsimx data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_lookup = Result.loc[:, ['Date', 'FactorialUID']].copy()\n",
    "Result.drop(columns=['Date', 'FactorialUID'])\n",
    "\n",
    "Result_list = list(Result)\n",
    "Result = torch.tensor(Result.to_numpy())\n",
    "\n",
    "Result_lookup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Result_lookup.shape)\n",
    "Result_lookup.merge(lookup_date.rename(columns={'Unix':'Date'}), how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Result_lookup.loc[(Result_lookup.FactorialUID == 24024), ].merge(lookup_date.rename(columns={'Unix':'Date'}), how = 'left').Date.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((19250-5285)/365)+1984"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Genotypes_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO make this valid for torch\n",
    "sow_date = lookup_date.loc[((lookup_date.Year == year) & \n",
    "                            (lookup_date.SowDate == lookup['SowDate'])), 'Unix'].values[0]\n",
    "\n",
    "# because of how this is set up index is also valid\n",
    "_ = lookup_date.loc[(lookup_date.Year == year), 'Unix'].agg(['min', 'max'])\n",
    "_['min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = (\n",
    "    (Result_lookup['Date'] >= _['min']) & \n",
    "    (Result_lookup['Date'] <= _['max']) &\n",
    "    (Result_lookup['FactorialUID'] == lookup['FactorialUID'])\n",
    "    )\n",
    "\n",
    "idx_Result = Result_lookup.loc[mask, ].index\n",
    "\n",
    "Result[idx_Result, ].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO make sure there aren't any values before the SowDate\n",
    "Result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 129486\n",
    "\n",
    "\n",
    "\n",
    "px.imshow((Result[(start-30):(start+30), 1:-1].numpy()[0:30, ]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "px.imshow(Result[idx_Result, ].numpy()[0:30, ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.zeros((365, 6))\n",
    "\n",
    "Result_lookup.loc[mask, ['Date']].min().values[0] - _['min']\n",
    "\n",
    "# sow_date - _['min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datetime.datetime(1970, 1, 1, 0, 0) + datetime.timedelta(10957)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# idx_Result.to_list()\n",
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Genotypes_cols"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
